{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CWtYF-UXKWP",
        "outputId": "dce8d435-3524-4393-8af4-149da868cc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://www.paddlepaddle.org.cn/packages/stable/cpu/\n",
            "Collecting paddlepaddle==3.2.0\n",
            "  Using cached https://paddle-whl.bj.bcebos.com/stable/cpu/paddlepaddle/paddlepaddle-3.2.0-cp312-cp312-linux_x86_64.whl (189.0 MB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (5.29.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (11.3.0)\n",
            "Requirement already satisfied: opt_einsum==3.3.0 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (3.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (4.15.0)\n",
            "Requirement already satisfied: safetensors>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.0) (0.6.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->paddlepaddle==3.2.0) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->paddlepaddle==3.2.0) (1.3.1)\n",
            "Installing collected packages: paddlepaddle\n",
            "  Attempting uninstall: paddlepaddle\n",
            "    Found existing installation: paddlepaddle 3.2.1\n",
            "    Uninstalling paddlepaddle-3.2.1:\n",
            "      Successfully uninstalled paddlepaddle-3.2.1\n",
            "Successfully installed paddlepaddle-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install paddlepaddle==3.2.0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddle2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb_4nMI-bdPt",
        "outputId": "0fbdf200-dfb4-46c5-a614-abb1c64db349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: paddle2onnx in /usr/local/lib/python3.12/dist-packages (2.0.1)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from paddle2onnx) (1.19.1)\n",
            "Requirement already satisfied: onnxoptimizer==0.3.13 in /usr/local/lib/python3.12/dist-packages (from paddle2onnx) (0.3.13)\n",
            "Requirement already satisfied: polygraphy>=0.49.20 in /usr/local/lib/python3.12/dist-packages (from paddle2onnx) (0.49.26)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->paddle2onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->paddle2onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->paddle2onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->paddle2onnx) (0.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PaddlePaddle/PaddleOCR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBZ9z0htXZUF",
        "outputId": "e513bc01-ac01-4c52-c90e-1b9d970ce322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PaddleOCR'...\n",
            "remote: Enumerating objects: 297716, done.\u001b[K\n",
            "remote: Counting objects: 100% (2105/2105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (433/433), done.\u001b[K\n",
            "remote: Total 297716 (delta 1872), reused 1729 (delta 1672), pack-reused 295611 (from 3)\u001b[K\n",
            "Receiving objects: 100% (297716/297716), 1.57 GiB | 25.36 MiB/s, done.\n",
            "Resolving deltas: 100% (235225/235225), done.\n",
            "Updating files: 100% (1991/1991), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://paddleocr.bj.bcebos.com/contribution/rec_r31_robustscanner.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk0Rer5eXulG",
        "outputId": "9bea966a-4436-4b98-d12f-ffccdb77deb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-08 08:28:10--  https://paddleocr.bj.bcebos.com/contribution/rec_r31_robustscanner.tar\n",
            "Resolving paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)... 103.235.47.176, 2402:2b40:7000:628:0:ff:b0e8:88da\n",
            "Connecting to paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)|103.235.47.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 579491840 (553M) [application/x-tar]\n",
            "Saving to: ‘rec_r31_robustscanner.tar’\n",
            "\n",
            "rec_r31_robustscann 100%[===================>] 552.65M  16.5MB/s    in 53s     \n",
            "\n",
            "2025-11-08 08:29:04 (10.4 MB/s) - ‘rec_r31_robustscanner.tar’ saved [579491840/579491840]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/export_model.py -c configs/rec/rec_resnet_rfl_visual.yml -o Global.pretrained_model=./rec_resnet_rfl_visual_train/best_accuracy  Global.save_inference_dir=./inference/rec_resnet_rfl_visual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLgTcjnAYOOF",
        "outputId": "3900ca81-1fcf-4752-9178-fedfbc1c3d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n",
            "Skipping import of the encryption module.\n",
            "[2025/11/08 08:04:46] ppocr INFO: load pretrain successful from ./rec_resnet_rfl_visual_train/best_accuracy\n",
            "[2025/11/08 08:04:46] ppocr INFO: Export inference config file to ./inference/rec_resnet_rfl_visual/inference.yml\n",
            "Skipping import of the encryption module\n",
            "W1108 08:04:47.443621 11196 eager_utils.cc:3441] Paddle static graph(PIR) not support input out tensor for now!!!!!\n",
            "[2025/11/08 08:04:48] ppocr INFO: inference model is saved to ./inference/rec_resnet_rfl_visual/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/PaddleOCR/rec_r31_robustscanner.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaloyY7UZCIk",
        "outputId": "f1d92443-df21-4f5c-c2f4-7a18690f6a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rec_r31_robustscanner/\n",
            "rec_r31_robustscanner/train.log\n",
            "rec_r31_robustscanner/best_accuracy.pdopt\n",
            "rec_r31_robustscanner/config.yml\n",
            "rec_r31_robustscanner/best_accuracy.states\n",
            "rec_r31_robustscanner/best_accuracy.pdparams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyclipper lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZv2OE93ZF3N",
        "outputId": "07974647-eb20-480f-aaf0-372b60a7b05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (1.3.0.post6)\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading lmdb-1.7.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!paddle2onnx --model_dir ./inference/rec_r31_robustscanner \\\n",
        "            --model_filename inference.json \\\n",
        "            --params_filename inference.pdiparams \\\n",
        "            --save_file model.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MVA-VkwZeBt",
        "outputId": "67b1db30-adbd-418b-cd63-470d1bf79740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n",
            "[Paddle2ONNX] Start parsing the Paddle model file...\n",
            "[Paddle2ONNX] Use opset_version = 17 for ONNX export.\n",
            "[Paddle2ONNX] PaddlePaddle model is exported as ONNX format now.\n",
            "2025-11-08 08:59:12 [INFO]\tTry to perform optimization on the ONNX model with onnxoptimizer.\n",
            "2025-11-08 09:05:02 [INFO]\tONNX model saved in model.onnx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/export_model.py -c configs/rec/rec_r31_robustscanner.yml -o Global.pretrained_model=./rec_r31_robustscanner/best_accuracy  Global.save_inference_dir=./inference/rec_r31_robustscanner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z477-e4UeShH",
        "outputId": "91d85b4d-0999-443b-c525-c2af3a38e770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n",
            "Skipping import of the encryption module.\n",
            "[2025/11/08 08:55:58] ppocr INFO: load pretrain successful from ./rec_r31_robustscanner/best_accuracy\n",
            "[2025/11/08 08:55:58] ppocr INFO: Export inference config file to ./inference/rec_r31_robustscanner/inference.yml\n",
            "Skipping import of the encryption module\n",
            "W1108 08:55:59.049072 23731 eager_utils.cc:3441] Paddle static graph(PIR) not support input out tensor for now!!!!!\n",
            "[2025/11/08 08:56:09] ppocr INFO: inference model is saved to ./inference/rec_r31_robustscanner/inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install paddlepaddle==3.2.1 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRrxQS5Rj_Je",
        "outputId": "8c1fa539-0e37-49c1-b56b-14fdc4b40a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://www.paddlepaddle.org.cn/packages/stable/cpu/\n",
            "Collecting paddlepaddle==3.2.1\n",
            "  Downloading https://paddle-whl.bj.bcebos.com/stable/cpu/paddlepaddle/paddlepaddle-3.2.1-cp312-cp312-linux_x86_64.whl (189.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (5.29.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (11.3.0)\n",
            "Requirement already satisfied: opt_einsum==3.3.0 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (3.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (3.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (4.15.0)\n",
            "Requirement already satisfied: safetensors>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle==3.2.1) (0.6.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.1) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.1) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.1) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->paddlepaddle==3.2.1) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->paddlepaddle==3.2.1) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->paddlepaddle==3.2.1) (1.3.1)\n",
            "Installing collected packages: paddlepaddle\n",
            "  Attempting uninstall: paddlepaddle\n",
            "    Found existing installation: paddlepaddle 3.2.0\n",
            "    Uninstalling paddlepaddle-3.2.0:\n",
            "      Successfully uninstalled paddlepaddle-3.2.0\n",
            "Successfully installed paddlepaddle-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['FLAGS_use_mkldnn'] = '0'"
      ],
      "metadata": {
        "id": "csWTtwrL4HCB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/infer/predict_rec.py \\\n",
        "  --image_dir=\"./joint.png\" \\\n",
        "  --rec_model_dir=\"./inference/rec_r31_robustscanner/\" \\\n",
        "  --rec_image_shape=\"3,48,48,160\" \\\n",
        "  --rec_algorithm=\"RobustScanner\" \\\n",
        "  --rec_char_dict_path=\"ppocr/utils/dict90.txt\" \\\n",
        "  --use_gpu=False \\\n",
        "  --use_space_char=False \\\n",
        "  --enable_mkldnn=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfMF_n60kkGU",
        "outputId": "6535db60-e4f8-4a56-fd74-bed24e22579a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n",
            "[2025/11/08 10:15:50] ppocr INFO: In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\n",
            "[2025/11/08 10:15:57] ppocr INFO: Predicts of ./joint.png:('joint', 0.9853368997573853)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "This code is refer from:\n",
        "https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/encoders/channel_reduction_encoder.py\n",
        "https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/robust_scanner_decoder.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import paddle\n",
        "from paddle import ParamAttr\n",
        "import paddle.nn as nn\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "\n",
        "class BaseDecoder(nn.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, img_metas):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward_test(self, feat, out_enc, img_metas):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        feat,\n",
        "        out_enc,\n",
        "        label=None,\n",
        "        valid_ratios=None,\n",
        "        word_positions=None,\n",
        "        train_mode=True,\n",
        "    ):\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        if train_mode:\n",
        "            return self.forward_train(\n",
        "                feat, out_enc, label, valid_ratios, word_positions\n",
        "            )\n",
        "        return self.forward_test(feat, out_enc, valid_ratios, word_positions)\n",
        "\n",
        "\n",
        "class ChannelReductionEncoder(nn.Layer):\n",
        "    \"\"\"Change the channel number with a one by one convoluational layer.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(ChannelReductionEncoder, self).__init__()\n",
        "\n",
        "        self.layer = nn.Conv2D(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            weight_attr=nn.initializer.XavierNormal(),\n",
        "        )\n",
        "\n",
        "    def forward(self, feat):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Image features with the shape of\n",
        "                :math:`(N, C_{in}, H, W)`.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A tensor of shape :math:`(N, C_{out}, H, W)`.\n",
        "        \"\"\"\n",
        "        return self.layer(feat)\n",
        "\n",
        "\n",
        "def masked_fill(x, mask, value):\n",
        "    y = paddle.full(x.shape, value, x.dtype)\n",
        "    return paddle.where(mask, y, x)\n",
        "\n",
        "\n",
        "class DotProductAttentionLayer(nn.Layer):\n",
        "    def __init__(self, dim_model=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.scale = dim_model**-0.5 if dim_model is not None else 1.0\n",
        "\n",
        "    def forward(self, query, key, value, h, w, valid_ratios=None):\n",
        "        # query: [N, D_m, Lq]\n",
        "        # key: [N, C_enc, Lk]  where Lk == h*w\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        logits = paddle.matmul(query, key) * self.scale\n",
        "        n, c, t = logits.shape\n",
        "        # reshape to (n, c, h, w)\n",
        "        logits = paddle.reshape(logits, [n, c, h, w])\n",
        "\n",
        "        if valid_ratios is not None:\n",
        "            # vectorized mask: set logits[..., valid_width:] = -inf per sample\n",
        "            # valid_ratios: Tensor of shape [N] or [N,1]\n",
        "            valid_ratios = paddle.reshape(valid_ratios, [-1])\n",
        "            widths = paddle.cast(paddle.round(valid_ratios * w), dtype='int64')\n",
        "            widths = paddle.clip(widths, min=0, max=w)\n",
        "\n",
        "            N = widths.shape[0]\n",
        "            W = logits.shape[-1]\n",
        "\n",
        "            arange = paddle.arange(W, dtype='int64')\n",
        "            arange = paddle.reshape(arange, [1, W])\n",
        "            arange = arange.expand([N, W])\n",
        "            widths_expand = paddle.reshape(widths, [N, 1]).expand([N, W])\n",
        "\n",
        "            mask = arange >= widths_expand  # [N, W]\n",
        "            mask = paddle.reshape(mask, [N, 1, 1, W])  # [N,1,1,W]\n",
        "\n",
        "            neg_inf = paddle.full([1], float('-inf'), dtype=logits.dtype)\n",
        "            # apply mask: positions >= valid_width -> -inf\n",
        "            logits = paddle.where(mask, neg_inf, logits)\n",
        "\n",
        "        # reshape to (n, c, t)\n",
        "        logits = paddle.reshape(logits, [n, c, t])\n",
        "        weights = F.softmax(logits, axis=2)\n",
        "        value = paddle.transpose(value, (0, 2, 1))\n",
        "        glimpse = paddle.matmul(weights, value)\n",
        "        glimpse = paddle.transpose(glimpse, (0, 2, 1))\n",
        "        return glimpse\n",
        "\n",
        "\n",
        "class SequenceAttentionDecoder(BaseDecoder):\n",
        "    \"\"\"Sequence attention decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        rnn_layers (int): Number of RNN layers.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        start_idx (int): The index of `<SOS>`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        padding_idx (int): The index of `<PAD>`.\n",
        "        dropout (float): Dropout rate.\n",
        "        return_feature (bool): Return feature or logits as the result.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss as specified in\n",
        "        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        rnn_layers=2,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        max_seq_len=40,\n",
        "        start_idx=0,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        dropout=0,\n",
        "        return_feature=False,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.return_feature = return_feature\n",
        "        self.encode_value = encode_value\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.start_idx = start_idx\n",
        "        self.mask = mask\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            self.num_classes, self.dim_model, padding_idx=padding_idx\n",
        "        )\n",
        "\n",
        "        self.sequence_layer = nn.LSTM(\n",
        "            input_size=dim_model,\n",
        "            hidden_size=dim_model,\n",
        "            num_layers=rnn_layers,\n",
        "            time_major=False,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.attention_layer = DotProductAttentionLayer()\n",
        "\n",
        "        self.prediction = None\n",
        "        if not self.return_feature:\n",
        "            pred_num_classes = num_classes - 1\n",
        "            self.prediction = nn.Linear(\n",
        "                dim_model if encode_value else dim_input, pred_num_classes\n",
        "            )\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, valid_ratios):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            targets (Tensor): a tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it would be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "\n",
        "        tgt_embedding = self.embedding(targets)\n",
        "\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, len_q, c_q = tgt_embedding.shape\n",
        "        assert c_q == self.dim_model\n",
        "        assert len_q <= self.max_seq_len\n",
        "\n",
        "        query, _ = self.sequence_layer(tgt_embedding)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n",
        "        if self.encode_value:\n",
        "            value = key\n",
        "        else:\n",
        "            value = paddle.reshape(feat, [n, c_feat, h * w])\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        out = self.prediction(attn_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output logit sequence tensor of shape\n",
        "            :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        seq_len = self.max_seq_len\n",
        "        batch_size = feat.shape[0]\n",
        "\n",
        "        decode_sequence = (\n",
        "            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n",
        "        )\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            step_out = self.forward_test_step(\n",
        "                feat, out_enc, decode_sequence, i, valid_ratios\n",
        "            )\n",
        "            outputs.append(step_out)\n",
        "            max_idx = paddle.argmax(step_out, axis=1, keepdim=False)\n",
        "            if i < seq_len - 1:\n",
        "                decode_sequence[:, i + 1] = max_idx\n",
        "\n",
        "        outputs = paddle.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward_test_step(\n",
        "        self, feat, out_enc, decode_sequence, current_step, valid_ratios\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            decode_sequence (Tensor): Shape :math:`(N, T)`. The tensor that\n",
        "                stores history decoding result.\n",
        "            current_step (int): Current decoding step.\n",
        "            valid_ratios (Tensor): valid length ratio of img\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Shape :math:`(N, C-1)`. The logit tensor of predicted\n",
        "            tokens at current time step.\n",
        "        \"\"\"\n",
        "\n",
        "        embed = self.embedding(decode_sequence)\n",
        "\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, _, c_q = embed.shape\n",
        "        assert c_q == self.dim_model\n",
        "\n",
        "        query, _ = self.sequence_layer(embed)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n",
        "        if self.encode_value:\n",
        "            value = key\n",
        "        else:\n",
        "            value = paddle.reshape(feat, [n, c_feat, h * w])\n",
        "\n",
        "        # [n, c, l]\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        out = attn_out[:, :, current_step]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return out\n",
        "\n",
        "        out = self.prediction(out)\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionAwareLayer(nn.Layer):\n",
        "    def __init__(self, dim_model, rnn_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=dim_model,\n",
        "            hidden_size=dim_model,\n",
        "            num_layers=rnn_layers,\n",
        "            time_major=False,\n",
        "        )\n",
        "\n",
        "        self.mixer = nn.Sequential(\n",
        "            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img_feature):\n",
        "        n, c, h, w = img_feature.shape\n",
        "        rnn_input = paddle.transpose(img_feature, (0, 2, 3, 1))\n",
        "        rnn_input = paddle.reshape(rnn_input, (n * h, w, c))\n",
        "        rnn_output, _ = self.rnn(rnn_input)\n",
        "        rnn_output = paddle.reshape(rnn_output, (n, h, w, c))\n",
        "        rnn_output = paddle.transpose(rnn_output, (0, 3, 1, 2))\n",
        "        out = self.mixer(rnn_output)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionAttentionDecoder(BaseDecoder):\n",
        "    \"\"\"Position attention decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        rnn_layers (int): Number of RNN layers.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        return_feature (bool): Return feature or logits as the result.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        rnn_layers=2,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        max_seq_len=40,\n",
        "        mask=True,\n",
        "        return_feature=False,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.return_feature = return_feature\n",
        "        self.encode_value = encode_value\n",
        "        self.mask = mask\n",
        "\n",
        "        self.embedding = nn.Embedding(self.max_seq_len + 1, self.dim_model)\n",
        "\n",
        "        self.position_aware_module = PositionAwareLayer(self.dim_model, rnn_layers)\n",
        "\n",
        "        self.attention_layer = DotProductAttentionLayer()\n",
        "\n",
        "        self.prediction = None\n",
        "        if not self.return_feature:\n",
        "            pred_num_classes = num_classes - 1\n",
        "            self.prediction = nn.Linear(\n",
        "                dim_model if encode_value else dim_input, pred_num_classes\n",
        "            )\n",
        "\n",
        "    def _get_position_index(self, length, batch_size):\n",
        "        position_index_list = []\n",
        "        for i in range(batch_size):\n",
        "            position_index = paddle.arange(0, end=length, step=1, dtype=\"int64\")\n",
        "            position_index_list.append(position_index)\n",
        "        batch_position_index = paddle.stack(position_index_list, axis=0)\n",
        "        return batch_position_index\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, valid_ratios, position_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            targets (dict): A dict with the key ``padded_targets``, a\n",
        "                tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "            position_index (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it will be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, len_q = targets.shape\n",
        "        assert len_q <= self.max_seq_len\n",
        "\n",
        "        position_out_enc = self.position_aware_module(out_enc)\n",
        "\n",
        "        query = self.embedding(position_index)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n",
        "        if self.encode_value:\n",
        "            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n",
        "        else:\n",
        "            value = paddle.reshape(feat, (n, c_feat, h * w))\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        return self.prediction(attn_out)\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios, position_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor): valid length ratio of img\n",
        "            position_index (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it would be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "\n",
        "        position_out_enc = self.position_aware_module(out_enc)\n",
        "\n",
        "        query = self.embedding(position_index)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n",
        "        if self.encode_value:\n",
        "            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n",
        "        else:\n",
        "            value = paddle.reshape(feat, (n, c_feat, h * w))\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        return self.prediction(attn_out)\n",
        "\n",
        "\n",
        "class RobustScannerFusionLayer(nn.Layer):\n",
        "    def __init__(self, dim_model, dim=-1):\n",
        "        super(RobustScannerFusionLayer, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim = dim\n",
        "        self.linear_layer = nn.Linear(dim_model * 2, dim_model * 2)\n",
        "\n",
        "    def forward(self, x0, x1):\n",
        "        # Only run Python-level shape assertion in dynamic mode (training/debug).\n",
        "        # During static export, evaluating tensor-based booleans causes dy2static\n",
        "        # to fail. So skip the assert in static mode.\n",
        "        if paddle.in_dynamic_mode():\n",
        "            assert x0.shape == x1.shape, f\"x0.shape ({x0.shape}) != x1.shape ({x1.shape})\"\n",
        "\n",
        "        fusion_input = paddle.concat([x0, x1], self.dim)\n",
        "        output = self.linear_layer(fusion_input)\n",
        "        output = F.glu(output, self.dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "class RobustScannerDecoder(BaseDecoder):\n",
        "    \"\"\"Decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        start_idx (int): The index of `<SOS>`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        padding_idx (int): The index of `<PAD>`.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss as specified in\n",
        "        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        hybrid_decoder_rnn_layers=2,\n",
        "        hybrid_decoder_dropout=0,\n",
        "        position_decoder_rnn_layers=2,\n",
        "        max_seq_len=40,\n",
        "        start_idx=0,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.encode_value = encode_value\n",
        "        self.start_idx = start_idx\n",
        "        self.padding_idx = padding_idx\n",
        "        self.mask = mask\n",
        "\n",
        "        # init hybrid decoder\n",
        "        self.hybrid_decoder = SequenceAttentionDecoder(\n",
        "            num_classes=num_classes,\n",
        "            rnn_layers=hybrid_decoder_rnn_layers,\n",
        "            dim_input=dim_input,\n",
        "            dim_model=dim_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            start_idx=start_idx,\n",
        "            mask=mask,\n",
        "            padding_idx=padding_idx,\n",
        "            dropout=hybrid_decoder_dropout,\n",
        "            encode_value=encode_value,\n",
        "            return_feature=True,\n",
        "        )\n",
        "\n",
        "        # init position decoder\n",
        "        self.position_decoder = PositionAttentionDecoder(\n",
        "            num_classes=num_classes,\n",
        "            rnn_layers=position_decoder_rnn_layers,\n",
        "            dim_input=dim_input,\n",
        "            dim_model=dim_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            mask=mask,\n",
        "            encode_value=encode_value,\n",
        "            return_feature=True,\n",
        "        )\n",
        "\n",
        "        self.fusion_module = RobustScannerFusionLayer(\n",
        "            self.dim_model if encode_value else dim_input\n",
        "        )\n",
        "\n",
        "        pred_num_classes = num_classes - 1\n",
        "        self.prediction = nn.Linear(\n",
        "            dim_model if encode_value else dim_input, pred_num_classes\n",
        "        )\n",
        "\n",
        "    def forward_train(self, feat, out_enc, target, valid_ratios, word_positions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            target (dict): A dict with the key ``padded_targets``, a\n",
        "                tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor):\n",
        "            word_positions (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        hybrid_glimpse = self.hybrid_decoder.forward_train(\n",
        "            feat, out_enc, target, valid_ratios\n",
        "        )\n",
        "        position_glimpse = self.position_decoder.forward_train(\n",
        "            feat, out_enc, target, valid_ratios, word_positions\n",
        "        )\n",
        "\n",
        "        fusion_out = self.fusion_module(hybrid_glimpse, position_glimpse)\n",
        "\n",
        "        out = self.prediction(fusion_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios, word_positions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor):\n",
        "            word_positions (Tensor): The position of each word.\n",
        "        Returns:\n",
        "            Tensor: The output logit sequence tensor of shape\n",
        "            :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        seq_len = self.max_seq_len\n",
        "        batch_size = feat.shape[0]\n",
        "\n",
        "        decode_sequence = (\n",
        "            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n",
        "        )\n",
        "\n",
        "        position_glimpse = self.position_decoder.forward_test(\n",
        "            feat, out_enc, valid_ratios, word_positions\n",
        "        )\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            hybrid_glimpse_step = self.hybrid_decoder.forward_test_step(\n",
        "                feat, out_enc, decode_sequence, i, valid_ratios\n",
        "            )\n",
        "\n",
        "            fusion_out = self.fusion_module(\n",
        "                hybrid_glimpse_step, position_glimpse[:, i, :]\n",
        "            )\n",
        "\n",
        "            char_out = self.prediction(fusion_out)\n",
        "            char_out = F.softmax(char_out, -1)\n",
        "            outputs.append(char_out)\n",
        "            max_idx = paddle.argmax(char_out, axis=1, keepdim=False)\n",
        "            if i < seq_len - 1:\n",
        "                decode_sequence[:, i + 1] = max_idx\n",
        "\n",
        "        outputs = paddle.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class RobustScannerHead(nn.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_channels,  # 90 + unknown + start + padding\n",
        "        in_channels,\n",
        "        enc_outchannles=128,\n",
        "        hybrid_dec_rnn_layers=2,\n",
        "        hybrid_dec_dropout=0,\n",
        "        position_dec_rnn_layers=2,\n",
        "        start_idx=0,\n",
        "        max_text_length=40,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        encode_value=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(RobustScannerHead, self).__init__()\n",
        "\n",
        "        # encoder module\n",
        "        self.encoder = ChannelReductionEncoder(\n",
        "            in_channels=in_channels, out_channels=enc_outchannles\n",
        "        )\n",
        "\n",
        "        # decoder module\n",
        "        self.decoder = RobustScannerDecoder(\n",
        "            num_classes=out_channels,\n",
        "            dim_input=in_channels,\n",
        "            dim_model=enc_outchannles,\n",
        "            hybrid_decoder_rnn_layers=hybrid_dec_rnn_layers,\n",
        "            hybrid_decoder_dropout=hybrid_dec_dropout,\n",
        "            position_decoder_rnn_layers=position_dec_rnn_layers,\n",
        "            max_seq_len=max_text_length,\n",
        "            start_idx=start_idx,\n",
        "            mask=mask,\n",
        "            padding_idx=padding_idx,\n",
        "            encode_value=encode_value,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        \"\"\"\n",
        "        targets: [label, valid_ratio, word_positions]\n",
        "        \"\"\"\n",
        "        out_enc = self.encoder(inputs)\n",
        "        valid_ratios = None\n",
        "        word_positions = targets[-1]\n",
        "\n",
        "        if len(targets) > 1:\n",
        "            valid_ratios = targets[-2]\n",
        "\n",
        "        if self.training:\n",
        "            label = targets[0]  # label\n",
        "            label = paddle.to_tensor(label, dtype=\"int64\")\n",
        "            final_out = self.decoder(\n",
        "                inputs, out_enc, label, valid_ratios, word_positions\n",
        "            )\n",
        "        else:\n",
        "            # For static export, if some tensor-based control flow still exists\n",
        "            # in downstream modules, fallback to disabling valid_ratios.\n",
        "            # But since attention is vectorized above, we pass valid_ratios as-is.\n",
        "            final_out = self.decoder(\n",
        "                inputs,\n",
        "                out_enc,\n",
        "                label=None,\n",
        "                valid_ratios=valid_ratios,\n",
        "                word_positions=word_positions,\n",
        "                train_mode=False,\n",
        "            )\n",
        "        return final_out\n",
        "'''\n",
        "with open('robust_scanner.py', 'w') as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "W-U4qPyyu3yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_predict_rec = '''# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "__dir__ = os.path.dirname(os.path.abspath(__file__))\n",
        "sys.path.append(__dir__)\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n",
        "\n",
        "os.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import traceback\n",
        "import paddle\n",
        "\n",
        "import tools.infer.utility as utility\n",
        "from ppocr.postprocess import build_post_process\n",
        "from ppocr.utils.logging import get_logger\n",
        "from ppocr.utils.utility import get_image_file_list, check_and_read\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "\n",
        "class TextRecognizer(object):\n",
        "    def __init__(self, args, logger=None):\n",
        "        if os.path.exists(f\"{args.rec_model_dir}/inference.yml\"):\n",
        "            model_config = utility.load_config(f\"{args.rec_model_dir}/inference.yml\")\n",
        "            model_name = model_config.get(\"Global\", {}).get(\"model_name\", \"\")\n",
        "            if model_name and model_name not in [\n",
        "                \"PP-OCRv5_mobile_rec\",\n",
        "                \"PP-OCRv5_server_rec\",\n",
        "                \"korean_PP-OCRv5_mobile_rec\",\n",
        "                \"eslav_PP-OCRv5_mobile_rec\",\n",
        "                \"latin_PP-OCRv5_mobile_rec\",\n",
        "                \"en_PP-OCRv5_mobile_rec\",\n",
        "                \"th_PP-OCRv5_mobile_rec\",\n",
        "                \"el_PP-OCRv5_mobile_rec\",\n",
        "            ]:\n",
        "                raise ValueError(\n",
        "                    f\"{model_name} is not supported. Please check if the model is supported by the PaddleOCR wheel.\"\n",
        "                )\n",
        "\n",
        "            if args.rec_char_dict_path == \"./ppocr/utils/ppocr_keys_v1.txt\":\n",
        "                rec_char_list = model_config.get(\"PostProcess\", {}).get(\n",
        "                    \"character_dict\", []\n",
        "                )\n",
        "                if rec_char_list:\n",
        "                    new_rec_char_dict_path = f\"{args.rec_model_dir}/ppocr_keys.txt\"\n",
        "                    with open(new_rec_char_dict_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.writelines([char + \"\\n\" for char in rec_char_list])\n",
        "                    args.rec_char_dict_path = new_rec_char_dict_path\n",
        "\n",
        "        if logger is None:\n",
        "            logger = get_logger()\n",
        "        self.rec_image_shape = [int(v) for v in args.rec_image_shape.split(\",\")]\n",
        "        self.rec_batch_num = args.rec_batch_num\n",
        "        self.rec_algorithm = args.rec_algorithm\n",
        "        postprocess_params = {\n",
        "            \"name\": \"CTCLabelDecode\",\n",
        "            \"character_dict_path\": args.rec_char_dict_path,\n",
        "            \"use_space_char\": args.use_space_char,\n",
        "        }\n",
        "        if self.rec_algorithm == \"SRN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SRNLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RARE\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"AttnLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"NRTR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"NRTRLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SAR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SARLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"VisionLAN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"VLLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"max_text_length\": args.max_text_length,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ViTSTR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ViTSTRLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ABINet\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ABINetLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SPIN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SPINLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RobustScanner\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SARLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RFL\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"RFLLabelDecode\",\n",
        "                \"character_dict_path\": None,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SATRN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SATRNLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm in [\"CPPD\", \"CPPDPadding\"]:\n",
        "            postprocess_params = {\n",
        "                \"name\": \"CPPDLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"PREN\":\n",
        "            postprocess_params = {\"name\": \"PRENLabelDecode\"}\n",
        "        elif self.rec_algorithm == \"CAN\":\n",
        "            self.inverse = args.rec_image_inverse\n",
        "            postprocess_params = {\n",
        "                \"name\": \"CANLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"LaTeXOCRDecode\",\n",
        "                \"rec_char_dict_path\": args.rec_char_dict_path,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ParseQ\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ParseQLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        self.postprocess_op = build_post_process(postprocess_params)\n",
        "        self.postprocess_params = postprocess_params\n",
        "        (\n",
        "            self.predictor,\n",
        "            self.input_tensor,\n",
        "            self.output_tensors,\n",
        "            self.config,\n",
        "        ) = utility.create_predictor(args, \"rec\", logger)\n",
        "        self.benchmark = args.benchmark\n",
        "        self.use_onnx = args.use_onnx\n",
        "        if args.benchmark:\n",
        "            import auto_log\n",
        "\n",
        "            pid = os.getpid()\n",
        "            gpu_id = utility.get_infer_gpuid()\n",
        "            self.autolog = auto_log.AutoLogger(\n",
        "                model_name=\"rec\",\n",
        "                model_precision=args.precision,\n",
        "                batch_size=args.rec_batch_num,\n",
        "                data_shape=\"dynamic\",\n",
        "                save_path=None,\n",
        "                inference_config=self.config,\n",
        "                pids=pid,\n",
        "                process_name=None,\n",
        "                gpu_ids=gpu_id if args.use_gpu else None,\n",
        "                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n",
        "                warmup=0,\n",
        "                logger=logger,\n",
        "            )\n",
        "        self.return_word_box = args.return_word_box\n",
        "\n",
        "    def resize_norm_img(self, img, max_wh_ratio):\n",
        "        imgC, imgH, imgW = self.rec_image_shape\n",
        "        if self.rec_algorithm == \"NRTR\" or self.rec_algorithm == \"ViTSTR\":\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            image_pil = Image.fromarray(np.uint8(img))\n",
        "            if self.rec_algorithm == \"ViTSTR\":\n",
        "                img = image_pil.resize([imgW, imgH], Image.BICUBIC)\n",
        "            else:\n",
        "                img = image_pil.resize([imgW, imgH], Image.Resampling.LANCZOS)\n",
        "            img = np.array(img)\n",
        "            norm_img = np.expand_dims(img, -1)\n",
        "            norm_img = norm_img.transpose((2, 0, 1))\n",
        "            if self.rec_algorithm == \"ViTSTR\":\n",
        "                norm_img = norm_img.astype(np.float32) / 255.0\n",
        "            else:\n",
        "                norm_img = norm_img.astype(np.float32) / 128.0 - 1.0\n",
        "            return norm_img\n",
        "        elif self.rec_algorithm == \"RFL\":\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_CUBIC)\n",
        "            resized_image = resized_image.astype(\"float32\")\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "            resized_image -= 0.5\n",
        "            resized_image /= 0.5\n",
        "            return resized_image\n",
        "\n",
        "        assert imgC == img.shape[2]\n",
        "        imgW = int((imgH * max_wh_ratio))\n",
        "        if self.use_onnx:\n",
        "            w = self.input_tensor.shape[3:][0]\n",
        "            if isinstance(w, str):\n",
        "                pass\n",
        "            elif w is not None and w > 0:\n",
        "                imgW = w\n",
        "        h, w = img.shape[:2]\n",
        "        ratio = w / float(h)\n",
        "        if math.ceil(imgH * ratio) > imgW:\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            resized_w = int(math.ceil(imgH * ratio))\n",
        "        if self.rec_algorithm == \"RARE\":\n",
        "            if resized_w > self.rec_image_shape[2]:\n",
        "                resized_w = self.rec_image_shape[2]\n",
        "            imgW = self.rec_image_shape[2]\n",
        "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_vl(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        img = img[:, :, ::-1]\n",
        "        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        return resized_image\n",
        "\n",
        "    def resize_norm_img_srn(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        img_black = np.zeros((imgH, imgW))\n",
        "        im_hei = img.shape[0]\n",
        "        im_wid = img.shape[1]\n",
        "        if im_wid <= im_hei * 1:\n",
        "            img_new = cv2.resize(img, (imgH * 1, imgH))\n",
        "        elif im_wid <= im_hei * 2:\n",
        "            img_new = cv2.resize(img, (imgH * 2, imgH))\n",
        "        elif im_wid <= im_hei * 3:\n",
        "            img_new = cv2.resize(img, (imgH * 3, imgH))\n",
        "        else:\n",
        "            img_new = cv2.resize(img, (imgW, imgH))\n",
        "        img_np = np.asarray(img_new)\n",
        "        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
        "        img_black[:, 0 : img_np.shape[1]] = img_np\n",
        "        img_black = img_black[:, :, np.newaxis]\n",
        "        row, col, c = img_black.shape\n",
        "        c = 1\n",
        "        return np.reshape(img_black, (c, row, col)).astype(np.float32)\n",
        "\n",
        "    def srn_other_inputs(self, image_shape, num_heads, max_text_length):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        feature_dim = int((imgH / 8) * (imgW / 8))\n",
        "        encoder_word_pos = np.array(range(0, feature_dim)).reshape((feature_dim, 1)).astype(\"int64\")\n",
        "        gsrm_word_pos = np.array(range(0, max_text_length)).reshape((max_text_length, 1)).astype(\"int64\")\n",
        "        gsrm_attn_bias_data = np.ones((1, max_text_length, max_text_length))\n",
        "        gsrm_slf_attn_bias1 = np.triu(gsrm_attn_bias_data, 1).reshape([-1, 1, max_text_length, max_text_length])\n",
        "        gsrm_slf_attn_bias1 = np.tile(gsrm_slf_attn_bias1, [1, num_heads, 1, 1]).astype(\"float32\") * [-1e9]\n",
        "        gsrm_slf_attn_bias2 = np.tril(gsrm_attn_bias_data, -1).reshape([-1, 1, max_text_length, max_text_length])\n",
        "        gsrm_slf_attn_bias2 = np.tile(gsrm_slf_attn_bias2, [1, num_heads, 1, 1]).astype(\"float32\") * [-1e9]\n",
        "        encoder_word_pos = encoder_word_pos[np.newaxis, :]\n",
        "        gsrm_word_pos = gsrm_word_pos[np.newaxis, :]\n",
        "        return [encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2]\n",
        "\n",
        "    def process_image_srn(self, img, image_shape, num_heads, max_text_length):\n",
        "        norm_img = self.resize_norm_img_srn(img, image_shape)\n",
        "        norm_img = norm_img[np.newaxis, :]\n",
        "        [encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2] = self.srn_other_inputs(image_shape, num_heads, max_text_length)\n",
        "        gsrm_slf_attn_bias1 = gsrm_slf_attn_bias1.astype(np.float32)\n",
        "        gsrm_slf_attn_bias2 = gsrm_slf_attn_bias2.astype(np.float32)\n",
        "        encoder_word_pos = encoder_word_pos.astype(np.int64)\n",
        "        gsrm_word_pos = gsrm_word_pos.astype(np.int64)\n",
        "        return (norm_img, encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2)\n",
        "\n",
        "    def resize_norm_img_sar(self, img, image_shape, width_downsample_ratio=0.25):\n",
        "        imgC, imgH, imgW_min, imgW_max = image_shape\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        valid_ratio = 1.0\n",
        "        width_divisor = int(1 / width_downsample_ratio)\n",
        "        ratio = w / float(h)\n",
        "        resize_w = math.ceil(imgH * ratio)\n",
        "        if resize_w % width_divisor != 0:\n",
        "            resize_w = round(resize_w / width_divisor) * width_divisor\n",
        "        if imgW_min is not None:\n",
        "            resize_w = max(imgW_min, resize_w)\n",
        "        if imgW_max is not None:\n",
        "            valid_ratio = min(1.0, 1.0 * resize_w / imgW_max)\n",
        "            resize_w = min(imgW_max, resize_w)\n",
        "        resized_image = cv2.resize(img, (resize_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        if image_shape[0] == 1:\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "        else:\n",
        "            resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        resize_shape = resized_image.shape\n",
        "        padding_im = -1.0 * np.ones((imgC, imgH, imgW_max), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resize_w] = resized_image\n",
        "        pad_shape = padding_im.shape\n",
        "        return padding_im, resize_shape, pad_shape, valid_ratio\n",
        "\n",
        "    def resize_norm_img_spin(self, img):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.resize(img, tuple([100, 32]), cv2.INTER_CUBIC)\n",
        "        img = np.array(img, np.float32)\n",
        "        img = np.expand_dims(img, -1)\n",
        "        img = img.transpose((2, 0, 1))\n",
        "        mean = [127.5]\n",
        "        std = [127.5]\n",
        "        mean = np.array(mean, dtype=np.float32)\n",
        "        std = np.array(std, dtype=np.float32)\n",
        "        mean = np.float32(mean.reshape(1, -1))\n",
        "        stdinv = 1 / np.float32(std.reshape(1, -1))\n",
        "        img -= mean\n",
        "        img *= stdinv\n",
        "        return img\n",
        "\n",
        "    def resize_norm_img_svtr(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        max_wh_ratio = imgW * 1.0 / imgH\n",
        "        h, w = img.shape[0], img.shape[1]\n",
        "        ratio = w * 1.0 / h\n",
        "        max_wh_ratio = min(max(max_wh_ratio, ratio), max_wh_ratio)\n",
        "        imgW = int(imgH * max_wh_ratio)\n",
        "        if math.ceil(imgH * ratio) > imgW:\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            resized_w = int(math.ceil(imgH * ratio))\n",
        "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_cppd_padding(self, img, image_shape, padding=True, interpolation=cv2.INTER_LINEAR):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        if not padding:\n",
        "            resized_image = cv2.resize(img, (imgW, imgH), interpolation=interpolation)\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            ratio = w / float(h)\n",
        "            if math.ceil(imgH * ratio) > imgW:\n",
        "                resized_w = imgW\n",
        "            else:\n",
        "                resized_w = int(math.ceil(imgH * ratio))\n",
        "            resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        if image_shape[0] == 1:\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "        else:\n",
        "            resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_abinet(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image / 255.0\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        resized_image = (resized_image - mean[None, None, ...]) / std[None, None, ...]\n",
        "        resized_image = resized_image.transpose((2, 0, 1))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        return resized_image\n",
        "\n",
        "    def norm_img_can(self, img, image_shape):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        if self.inverse:\n",
        "            img = 255 - img\n",
        "        if self.rec_image_shape[0] == 1:\n",
        "            h, w = img.shape\n",
        "            _, imgH, imgW = self.rec_image_shape\n",
        "            if h < imgH or w < imgW:\n",
        "                padding_h = max(imgH - h, 0)\n",
        "                padding_w = max(imgW - w, 0)\n",
        "                img_padded = np.pad(img, ((0, padding_h), (0, padding_w)), \"constant\", constant_values=(255))\n",
        "                img = img_padded\n",
        "        img = np.expand_dims(img, 0) / 255.0\n",
        "        img = img.astype(\"float32\")\n",
        "        return img\n",
        "\n",
        "    def pad_(self, img, divable=32):\n",
        "        threshold = 128\n",
        "        data = np.array(img.convert(\"LA\"))\n",
        "        if data[..., -1].var() == 0:\n",
        "            data = (data[..., 0]).astype(np.uint8)\n",
        "        else:\n",
        "            data = (255 - data[..., -1]).astype(np.uint8)\n",
        "        data = (data - data.min()) / (data.max() - data.min()) * 255\n",
        "        if data.mean() > threshold:\n",
        "            gray = 255 * (data < threshold).astype(np.uint8)\n",
        "        else:\n",
        "            gray = 255 * (data > threshold).astype(np.uint8)\n",
        "            data = 255 - data\n",
        "        coords = cv2.findNonZero(gray)\n",
        "        a, b, w, h = cv2.boundingRect(coords)\n",
        "        rect = data[b : b + h, a : a + w]\n",
        "        im = Image.fromarray(rect).convert(\"L\")\n",
        "        dims = []\n",
        "        for x in [w, h]:\n",
        "            div, mod = divmod(x, divable)\n",
        "            dims.append(divable * (div + (1 if mod > 0 else 0)))\n",
        "        padded = Image.new(\"L\", dims, 255)\n",
        "        padded.paste(im, (0, 0, im.size[0], im.size[1]))\n",
        "        return padded\n",
        "\n",
        "    def minmax_size_(self, img, max_dimensions, min_dimensions):\n",
        "        if max_dimensions is not None:\n",
        "            ratios = [a / b for a, b in zip(img.size, max_dimensions)]\n",
        "            if any([r > 1 for r in ratios]):\n",
        "                size = np.array(img.size) // max(ratios)\n",
        "                img = img.resize(tuple(size.astype(int)), Image.BILINEAR)\n",
        "        if min_dimensions is not None:\n",
        "            padded_size = [max(img_dim, min_dim) for img_dim, min_dim in zip(img.size, min_dimensions)]\n",
        "            if padded_size != list(img.size):\n",
        "                padded_im = Image.new(\"L\", padded_size, 255)\n",
        "                padded_im.paste(img, img.getbbox())\n",
        "                img = padded_im\n",
        "        return img\n",
        "\n",
        "    def norm_img_latexocr(self, img):\n",
        "        shape = (1, 1, 3)\n",
        "        mean = [0.7931, 0.7931, 0.7931]\n",
        "        std = [0.1738, 0.1738, 0.1738]\n",
        "        scale = np.float32(1.0 / 255.0)\n",
        "        min_dimensions = [32, 32]\n",
        "        max_dimensions = [672, 192]\n",
        "        mean = np.array(mean).reshape(shape).astype(\"float32\")\n",
        "        std = np.array(std).reshape(shape).astype(\"float32\")\n",
        "        im_h, im_w = img.shape[:2]\n",
        "        if not (min_dimensions[0] <= im_w <= max_dimensions[0] and min_dimensions[1] <= im_h <= max_dimensions[1]):\n",
        "            img = Image.fromarray(np.uint8(img))\n",
        "            img = self.minmax_size_(self.pad_(img), max_dimensions, min_dimensions)\n",
        "            img = np.array(img)\n",
        "            im_h, im_w = img.shape[:2]\n",
        "            img = np.dstack([img, img, img])\n",
        "        img = (img.astype(\"float32\") * scale - mean) / std\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        divide_h = math.ceil(im_h / 16) * 16\n",
        "        divide_w = math.ceil(im_w / 16) * 16\n",
        "        img = np.pad(img, ((0, divide_h - im_h), (0, divide_w - im_w)), constant_values=(1, 1))\n",
        "        img = img[:, :, np.newaxis].transpose(2, 0, 1)\n",
        "        img = img.astype(\"float32\")\n",
        "        return img\n",
        "\n",
        "    def __call__(self, img_list):\n",
        "        img_num = len(img_list)\n",
        "        width_list = [img.shape[1] / float(img.shape[0]) for img in img_list]\n",
        "        indices = np.argsort(np.array(width_list))\n",
        "        rec_res = [[\"\", 0.0]] * img_num\n",
        "        batch_num = self.rec_batch_num\n",
        "        st = time.time()\n",
        "        if self.benchmark:\n",
        "            self.autolog.times.start()\n",
        "\n",
        "        for beg_img_no in range(0, img_num, batch_num):\n",
        "            end_img_no = min(img_num, beg_img_no + batch_num)\n",
        "            norm_img_batch = []\n",
        "            if self.rec_algorithm == \"SRN\":\n",
        "                encoder_word_pos_list = []\n",
        "                gsrm_word_pos_list = []\n",
        "                gsrm_slf_attn_bias1_list = []\n",
        "                gsrm_slf_attn_bias2_list = []\n",
        "            if self.rec_algorithm == \"SAR\":\n",
        "                valid_ratios = []\n",
        "            imgC, imgH, imgW = self.rec_image_shape[:3]\n",
        "            max_wh_ratio = imgW / imgH\n",
        "            wh_ratio_list = []\n",
        "            for ino in range(beg_img_no, end_img_no):\n",
        "                h, w = img_list[indices[ino]].shape[0:2]\n",
        "                wh_ratio = w * 1.0 / h\n",
        "                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
        "                wh_ratio_list.append(wh_ratio)\n",
        "\n",
        "            for ino in range(beg_img_no, end_img_no):\n",
        "                if self.rec_algorithm == \"SAR\":\n",
        "                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n",
        "                    valid_ratios.append(valid_ratio)\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"SRN\":\n",
        "                    norm_img = self.process_image_srn(img_list[indices[ino]], self.rec_image_shape, 8, 25)\n",
        "                    encoder_word_pos_list.append(norm_img[1])\n",
        "                    gsrm_word_pos_list.append(norm_img[2])\n",
        "                    gsrm_slf_attn_bias1_list.append(norm_img[3])\n",
        "                    gsrm_slf_attn_bias2_list.append(norm_img[4])\n",
        "                    norm_img_batch.append(norm_img[0])\n",
        "                elif self.rec_algorithm in [\"SVTR\", \"SATRN\", \"ParseQ\", \"CPPD\"]:\n",
        "                    norm_img = self.resize_norm_img_svtr(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm in [\"CPPDPadding\"]:\n",
        "                    norm_img = self.resize_norm_img_cppd_padding(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm in [\"VisionLAN\", \"PREN\"]:\n",
        "                    norm_img = self.resize_norm_img_vl(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"SPIN\":\n",
        "                    norm_img = self.resize_norm_img_spin(img_list[indices[ino]])\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"ABINet\":\n",
        "                    norm_img = self.resize_norm_img_abinet(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"RobustScanner\":\n",
        "                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(\n",
        "                        img_list[indices[ino]],\n",
        "                        self.rec_image_shape,\n",
        "                        width_downsample_ratio=0.25,\n",
        "                    )\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n",
        "                    valid_ratios = [] if 'valid_ratios' not in locals() else valid_ratios\n",
        "                    valid_ratios.append(valid_ratio)\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                    word_positions_list = [] if 'word_positions_list' not in locals() else word_positions_list\n",
        "                    word_positions = np.array(range(0, 40)).astype(\"int64\")\n",
        "                    word_positions = np.expand_dims(word_positions, axis=0)\n",
        "                    word_positions_list.append(word_positions)\n",
        "                elif self.rec_algorithm == \"CAN\":\n",
        "                    norm_img = self.norm_img_can(img_list[indices[ino]], max_wh_ratio)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                    norm_image_mask = np.ones(norm_img.shape, dtype=\"float32\")\n",
        "                    word_label = np.ones([1, 36], dtype=\"int64\")\n",
        "                    norm_img_mask_batch = [] if 'norm_img_mask_batch' not in locals() else norm_img_mask_batch\n",
        "                    word_label_list = [] if 'word_label_list' not in locals() else word_label_list\n",
        "                    norm_img_mask_batch.append(norm_image_mask)\n",
        "                    word_label_list.append(word_label)\n",
        "                elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "                    norm_img = self.norm_img_latexocr(img_list[indices[ino]])\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                else:\n",
        "                    norm_img = self.resize_norm_img(img_list[indices[ino]], max_wh_ratio)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "\n",
        "            norm_img_batch = np.concatenate(norm_img_batch)\n",
        "            norm_img_batch = norm_img_batch.copy()\n",
        "            norm_img_batch = norm_img_batch.astype(np.float32)  # FORCE float32\n",
        "\n",
        "            if self.benchmark:\n",
        "                self.autolog.times.stamp()\n",
        "\n",
        "            if self.rec_algorithm == \"SRN\":\n",
        "                encoder_word_pos_list = np.concatenate(encoder_word_pos_list)\n",
        "                gsrm_word_pos_list = np.concatenate(gsrm_word_pos_list)\n",
        "                gsrm_slf_attn_bias1_list = np.concatenate(gsrm_slf_attn_bias1_list)\n",
        "                gsrm_slf_attn_bias2_list = np.concatenate(gsrm_slf_attn_bias2_list)\n",
        "                inputs = [norm_img_batch, encoder_word_pos_list, gsrm_word_pos_list, gsrm_slf_attn_bias1_list, gsrm_slf_attn_bias2_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = {\"predict\": outputs[2]}\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = {\"predict\": outputs[2]}\n",
        "            elif self.rec_algorithm == \"SAR\":\n",
        "                valid_ratios = np.concatenate(valid_ratios).astype(np.float32)\n",
        "                inputs = [norm_img_batch, np.array([valid_ratios], dtype=np.float32).T]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0]\n",
        "            elif self.rec_algorithm == \"RobustScanner\":\n",
        "                valid_ratios = np.concatenate(valid_ratios).astype(np.float32)\n",
        "                word_positions_list = np.concatenate(word_positions_list).astype(np.int64)\n",
        "                inputs = [norm_img_batch, valid_ratios, word_positions_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0]\n",
        "            elif self.rec_algorithm == \"CAN\":\n",
        "                norm_img_mask_batch = np.concatenate(norm_img_mask_batch)\n",
        "                word_label_list = np.concatenate(word_label_list)\n",
        "                inputs = [norm_img_batch, norm_img_mask_batch, word_label_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs\n",
        "            elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "                inputs = [norm_img_batch]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs\n",
        "            else:\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    self.input_tensor.copy_from_cpu(norm_img_batch)\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0] if len(outputs) == 1 else outputs\n",
        "\n",
        "            if self.postprocess_params[\"name\"] == \"CTCLabelDecode\":\n",
        "                rec_result = self.postprocess_op(preds, return_word_box=self.return_word_box, wh_ratio_list=wh_ratio_list, max_wh_ratio=max_wh_ratio)\n",
        "            elif self.postprocess_params[\"name\"] == \"LaTeXOCRDecode\":\n",
        "                preds = [p.reshape([-1]) for p in preds]\n",
        "                rec_result = self.postprocess_op(preds)\n",
        "            else:\n",
        "                rec_result = self.postprocess_op(preds)\n",
        "\n",
        "            for rno in range(len(rec_result)):\n",
        "                rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n",
        "            if self.benchmark:\n",
        "                self.autolog.times.end(stamp=True)\n",
        "\n",
        "        return rec_res, time.time() - st\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    image_file_list = get_image_file_list(args.image_dir)\n",
        "    valid_image_file_list = []\n",
        "    img_list = []\n",
        "    log_file = args.save_log_path\n",
        "    if os.path.isdir(args.save_log_path) or (not os.path.exists(args.save_log_path) and args.save_log_path.endswith(\"/\")):\n",
        "        log_file = os.path.join(log_file, \"benchmark_recognition.log\")\n",
        "    logger = get_logger(log_file=log_file)\n",
        "    logger.info(\"In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\")\n",
        "    text_recognizer = TextRecognizer(args)\n",
        "    if args.warmup:\n",
        "        img = np.random.uniform(0, 255, [48, 320, 3]).astype(np.uint8)\n",
        "        for i in range(2):\n",
        "            res = text_recognizer([img] * int(args.rec_batch_num))\n",
        "    for image_file in image_file_list:\n",
        "        img, flag, _ = check_and_read(image_file)\n",
        "        if not flag:\n",
        "            img = cv2.imread(image_file)\n",
        "        if img is None:\n",
        "            logger.info(\"error in loading image:{}\".format(image_file))\n",
        "            continue\n",
        "        valid_image_file_list.append(image_file)\n",
        "        img_list.append(img)\n",
        "    try:\n",
        "        rec_res, _ = text_recognizer(img_list)\n",
        "    except Exception as E:\n",
        "        logger.info(traceback.format_exc())\n",
        "        logger.info(E)\n",
        "        exit()\n",
        "    for ino in range(len(img_list)):\n",
        "        logger.info(\"Predicts of {}:{}\".format(valid_image_file_list[ino], rec_res[ino]))\n",
        "    if args.benchmark:\n",
        "        text_recognizer.autolog.report()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(utility.parse_args())'''\n"
      ],
      "metadata": {
        "id": "d0cZBxQQ8tpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "__dir__ = os.path.dirname(os.path.abspath(__file__))\n",
        "sys.path.append(__dir__)\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n",
        "\n",
        "os.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import traceback\n",
        "import paddle\n",
        "\n",
        "import tools.infer.utility as utility\n",
        "from ppocr.postprocess import build_post_process\n",
        "from ppocr.utils.logging import get_logger\n",
        "from ppocr.utils.utility import get_image_file_list, check_and_read\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "\n",
        "class TextRecognizer(object):\n",
        "    def __init__(self, args, logger=None):\n",
        "        if os.path.exists(f\"{args.rec_model_dir}/inference.yml\"):\n",
        "            model_config = utility.load_config(f\"{args.rec_model_dir}/inference.yml\")\n",
        "            model_name = model_config.get(\"Global\", {}).get(\"model_name\", \"\")\n",
        "            if model_name and model_name not in [\n",
        "                \"PP-OCRv5_mobile_rec\",\n",
        "                \"PP-OCRv5_server_rec\",\n",
        "                \"korean_PP-OCRv5_mobile_rec\",\n",
        "                \"eslav_PP-OCRv5_mobile_rec\",\n",
        "                \"latin_PP-OCRv5_mobile_rec\",\n",
        "                \"en_PP-OCRv5_mobile_rec\",\n",
        "                \"th_PP-OCRv5_mobile_rec\",\n",
        "                \"el_PP-OCRv5_mobile_rec\",\n",
        "            ]:\n",
        "                raise ValueError(\n",
        "                    f\"{model_name} is not supported. Please check if the model is supported by the PaddleOCR wheel.\"\n",
        "                )\n",
        "\n",
        "            if args.rec_char_dict_path == \"./ppocr/utils/ppocr_keys_v1.txt\":\n",
        "                rec_char_list = model_config.get(\"PostProcess\", {}).get(\n",
        "                    \"character_dict\", []\n",
        "                )\n",
        "                if rec_char_list:\n",
        "                    new_rec_char_dict_path = f\"{args.rec_model_dir}/ppocr_keys.txt\"\n",
        "                    with open(new_rec_char_dict_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.writelines([char + \"\\n\" for char in rec_char_list])\n",
        "                    args.rec_char_dict_path = new_rec_char_dict_path\n",
        "\n",
        "        if logger is None:\n",
        "            logger = get_logger()\n",
        "        self.rec_image_shape = [int(v) for v in args.rec_image_shape.split(\",\")]\n",
        "        self.rec_batch_num = args.rec_batch_num\n",
        "        self.rec_algorithm = args.rec_algorithm\n",
        "        postprocess_params = {\n",
        "            \"name\": \"CTCLabelDecode\",\n",
        "            \"character_dict_path\": args.rec_char_dict_path,\n",
        "            \"use_space_char\": args.use_space_char,\n",
        "        }\n",
        "        if self.rec_algorithm == \"SRN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SRNLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RARE\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"AttnLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"NRTR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"NRTRLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SAR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SARLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"VisionLAN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"VLLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"max_text_length\": args.max_text_length,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ViTSTR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ViTSTRLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ABINet\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ABINetLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SPIN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SPINLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RobustScanner\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SARLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"RFL\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"RFLLabelDecode\",\n",
        "                \"character_dict_path\": None,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"SATRN\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"SATRNLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm in [\"CPPD\", \"CPPDPadding\"]:\n",
        "            postprocess_params = {\n",
        "                \"name\": \"CPPDLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "                \"rm_symbol\": True,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"PREN\":\n",
        "            postprocess_params = {\"name\": \"PRENLabelDecode\"}\n",
        "        elif self.rec_algorithm == \"CAN\":\n",
        "            self.inverse = args.rec_image_inverse\n",
        "            postprocess_params = {\n",
        "                \"name\": \"CANLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"LaTeXOCRDecode\",\n",
        "                \"rec_char_dict_path\": args.rec_char_dict_path,\n",
        "            }\n",
        "        elif self.rec_algorithm == \"ParseQ\":\n",
        "            postprocess_params = {\n",
        "                \"name\": \"ParseQLabelDecode\",\n",
        "                \"character_dict_path\": args.rec_char_dict_path,\n",
        "                \"use_space_char\": args.use_space_char,\n",
        "            }\n",
        "        self.postprocess_op = build_post_process(postprocess_params)\n",
        "        self.postprocess_params = postprocess_params\n",
        "        (\n",
        "            self.predictor,\n",
        "            self.input_tensor,\n",
        "            self.output_tensors,\n",
        "            self.config,\n",
        "        ) = utility.create_predictor(args, \"rec\", logger)\n",
        "        self.benchmark = args.benchmark\n",
        "        self.use_onnx = args.use_onnx\n",
        "        if args.benchmark:\n",
        "            import auto_log\n",
        "\n",
        "            pid = os.getpid()\n",
        "            gpu_id = utility.get_infer_gpuid()\n",
        "            self.autolog = auto_log.AutoLogger(\n",
        "                model_name=\"rec\",\n",
        "                model_precision=args.precision,\n",
        "                batch_size=args.rec_batch_num,\n",
        "                data_shape=\"dynamic\",\n",
        "                save_path=None,\n",
        "                inference_config=self.config,\n",
        "                pids=pid,\n",
        "                process_name=None,\n",
        "                gpu_ids=gpu_id if args.use_gpu else None,\n",
        "                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n",
        "                warmup=0,\n",
        "                logger=logger,\n",
        "            )\n",
        "        self.return_word_box = args.return_word_box\n",
        "\n",
        "    def resize_norm_img(self, img, max_wh_ratio):\n",
        "        imgC, imgH, imgW = self.rec_image_shape\n",
        "        if self.rec_algorithm == \"NRTR\" or self.rec_algorithm == \"ViTSTR\":\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            image_pil = Image.fromarray(np.uint8(img))\n",
        "            if self.rec_algorithm == \"ViTSTR\":\n",
        "                img = image_pil.resize([imgW, imgH], Image.BICUBIC)\n",
        "            else:\n",
        "                img = image_pil.resize([imgW, imgH], Image.Resampling.LANCZOS)\n",
        "            img = np.array(img)\n",
        "            norm_img = np.expand_dims(img, -1)\n",
        "            norm_img = norm_img.transpose((2, 0, 1))\n",
        "            if self.rec_algorithm == \"ViTSTR\":\n",
        "                norm_img = norm_img.astype(np.float32) / 255.0\n",
        "            else:\n",
        "                norm_img = norm_img.astype(np.float32) / 128.0 - 1.0\n",
        "            return norm_img\n",
        "        elif self.rec_algorithm == \"RFL\":\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_CUBIC)\n",
        "            resized_image = resized_image.astype(\"float32\")\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "            resized_image -= 0.5\n",
        "            resized_image /= 0.5\n",
        "            return resized_image\n",
        "\n",
        "        assert imgC == img.shape[2]\n",
        "        imgW = int((imgH * max_wh_ratio))\n",
        "        if self.use_onnx:\n",
        "            w = self.input_tensor.shape[3:][0]\n",
        "            if isinstance(w, str):\n",
        "                pass\n",
        "            elif w is not None and w > 0:\n",
        "                imgW = w\n",
        "        h, w = img.shape[:2]\n",
        "        ratio = w / float(h)\n",
        "        if math.ceil(imgH * ratio) > imgW:\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            resized_w = int(math.ceil(imgH * ratio))\n",
        "        if self.rec_algorithm == \"RARE\":\n",
        "            if resized_w > self.rec_image_shape[2]:\n",
        "                resized_w = self.rec_image_shape[2]\n",
        "            imgW = self.rec_image_shape[2]\n",
        "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_vl(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        img = img[:, :, ::-1]\n",
        "        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        return resized_image\n",
        "\n",
        "    def resize_norm_img_srn(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        img_black = np.zeros((imgH, imgW))\n",
        "        im_hei = img.shape[0]\n",
        "        im_wid = img.shape[1]\n",
        "        if im_wid <= im_hei * 1:\n",
        "            img_new = cv2.resize(img, (imgH * 1, imgH))\n",
        "        elif im_wid <= im_hei * 2:\n",
        "            img_new = cv2.resize(img, (imgH * 2, imgH))\n",
        "        elif im_wid <= im_hei * 3:\n",
        "            img_new = cv2.resize(img, (imgH * 3, imgH))\n",
        "        else:\n",
        "            img_new = cv2.resize(img, (imgW, imgH))\n",
        "        img_np = np.asarray(img_new)\n",
        "        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
        "        img_black[:, 0 : img_np.shape[1]] = img_np\n",
        "        img_black = img_black[:, :, np.newaxis]\n",
        "        row, col, c = img_black.shape\n",
        "        c = 1\n",
        "        return np.reshape(img_black, (c, row, col)).astype(np.float32)\n",
        "\n",
        "    def srn_other_inputs(self, image_shape, num_heads, max_text_length):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        feature_dim = int((imgH / 8) * (imgW / 8))\n",
        "        encoder_word_pos = np.array(range(0, feature_dim)).reshape((feature_dim, 1)).astype(\"int64\")\n",
        "        gsrm_word_pos = np.array(range(0, max_text_length)).reshape((max_text_length, 1)).astype(\"int64\")\n",
        "        gsrm_attn_bias_data = np.ones((1, max_text_length, max_text_length))\n",
        "        gsrm_slf_attn_bias1 = np.triu(gsrm_attn_bias_data, 1).reshape([-1, 1, max_text_length, max_text_length])\n",
        "        gsrm_slf_attn_bias1 = np.tile(gsrm_slf_attn_bias1, [1, num_heads, 1, 1]).astype(\"float32\") * [-1e9]\n",
        "        gsrm_slf_attn_bias2 = np.tril(gsrm_attn_bias_data, -1).reshape([-1, 1, max_text_length, max_text_length])\n",
        "        gsrm_slf_attn_bias2 = np.tile(gsrm_slf_attn_bias2, [1, num_heads, 1, 1]).astype(\"float32\") * [-1e9]\n",
        "        encoder_word_pos = encoder_word_pos[np.newaxis, :]\n",
        "        gsrm_word_pos = gsrm_word_pos[np.newaxis, :]\n",
        "        return [encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2]\n",
        "\n",
        "    def process_image_srn(self, img, image_shape, num_heads, max_text_length):\n",
        "        norm_img = self.resize_norm_img_srn(img, image_shape)\n",
        "        norm_img = norm_img[np.newaxis, :]\n",
        "        [encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2] = self.srn_other_inputs(image_shape, num_heads, max_text_length)\n",
        "        gsrm_slf_attn_bias1 = gsrm_slf_attn_bias1.astype(np.float32)\n",
        "        gsrm_slf_attn_bias2 = gsrm_slf_attn_bias2.astype(np.float32)\n",
        "        encoder_word_pos = encoder_word_pos.astype(np.int64)\n",
        "        gsrm_word_pos = gsrm_word_pos.astype(np.int64)\n",
        "        return (norm_img, encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2)\n",
        "\n",
        "    def resize_norm_img_sar(self, img, image_shape, width_downsample_ratio=0.25):\n",
        "        imgC, imgH, imgW_min, imgW_max = image_shape\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        valid_ratio = 1.0\n",
        "        width_divisor = int(1 / width_downsample_ratio)\n",
        "        ratio = w / float(h)\n",
        "        resize_w = math.ceil(imgH * ratio)\n",
        "        if resize_w % width_divisor != 0:\n",
        "            resize_w = round(resize_w / width_divisor) * width_divisor\n",
        "        if imgW_min is not None:\n",
        "            resize_w = max(imgW_min, resize_w)\n",
        "        if imgW_max is not None:\n",
        "            valid_ratio = min(1.0, 1.0 * resize_w / imgW_max)\n",
        "            resize_w = min(imgW_max, resize_w)\n",
        "        resized_image = cv2.resize(img, (resize_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        if image_shape[0] == 1:\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "        else:\n",
        "            resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        resize_shape = resized_image.shape\n",
        "        padding_im = -1.0 * np.ones((imgC, imgH, imgW_max), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resize_w] = resized_image\n",
        "        pad_shape = padding_im.shape\n",
        "        return padding_im, resize_shape, pad_shape, valid_ratio\n",
        "\n",
        "    def resize_norm_img_spin(self, img):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.resize(img, tuple([100, 32]), cv2.INTER_CUBIC)\n",
        "        img = np.array(img, np.float32)\n",
        "        img = np.expand_dims(img, -1)\n",
        "        img = img.transpose((2, 0, 1))\n",
        "        mean = [127.5]\n",
        "        std = [127.5]\n",
        "        mean = np.array(mean, dtype=np.float32)\n",
        "        std = np.array(std, dtype=np.float32)\n",
        "        mean = np.float32(mean.reshape(1, -1))\n",
        "        stdinv = 1 / np.float32(std.reshape(1, -1))\n",
        "        img -= mean\n",
        "        img *= stdinv\n",
        "        return img\n",
        "\n",
        "    def resize_norm_img_svtr(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        max_wh_ratio = imgW * 1.0 / imgH\n",
        "        h, w = img.shape[0], img.shape[1]\n",
        "        ratio = w * 1.0 / h\n",
        "        max_wh_ratio = min(max(max_wh_ratio, ratio), max_wh_ratio)\n",
        "        imgW = int(imgH * max_wh_ratio)\n",
        "        if math.ceil(imgH * ratio) > imgW:\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            resized_w = int(math.ceil(imgH * ratio))\n",
        "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_cppd_padding(self, img, image_shape, padding=True, interpolation=cv2.INTER_LINEAR):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        h = img.shape[0]\n",
        "        w = img.shape[1]\n",
        "        if not padding:\n",
        "            resized_image = cv2.resize(img, (imgW, imgH), interpolation=interpolation)\n",
        "            resized_w = imgW\n",
        "        else:\n",
        "            ratio = w / float(h)\n",
        "            if math.ceil(imgH * ratio) > imgW:\n",
        "                resized_w = imgW\n",
        "            else:\n",
        "                resized_w = int(math.ceil(imgH * ratio))\n",
        "            resized_image = cv2.resize(img, (resized_w, imgH))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        if image_shape[0] == 1:\n",
        "            resized_image = resized_image / 255\n",
        "            resized_image = resized_image[np.newaxis, :]\n",
        "        else:\n",
        "            resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
        "        resized_image -= 0.5\n",
        "        resized_image /= 0.5\n",
        "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
        "        padding_im[:, :, 0:resized_w] = resized_image\n",
        "        return padding_im\n",
        "\n",
        "    def resize_norm_img_abinet(self, img, image_shape):\n",
        "        imgC, imgH, imgW = image_shape\n",
        "        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        resized_image = resized_image / 255.0\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        resized_image = (resized_image - mean[None, None, ...]) / std[None, None, ...]\n",
        "        resized_image = resized_image.transpose((2, 0, 1))\n",
        "        resized_image = resized_image.astype(\"float32\")\n",
        "        return resized_image\n",
        "\n",
        "    def norm_img_can(self, img, image_shape):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        if self.inverse:\n",
        "            img = 255 - img\n",
        "        if self.rec_image_shape[0] == 1:\n",
        "            h, w = img.shape\n",
        "            _, imgH, imgW = self.rec_image_shape\n",
        "            if h < imgH or w < imgW:\n",
        "                padding_h = max(imgH - h, 0)\n",
        "                padding_w = max(imgW - w, 0)\n",
        "                img_padded = np.pad(img, ((0, padding_h), (0, padding_w)), \"constant\", constant_values=(255))\n",
        "                img = img_padded\n",
        "        img = np.expand_dims(img, 0) / 255.0\n",
        "        img = img.astype(\"float32\")\n",
        "        return img\n",
        "\n",
        "    def pad_(self, img, divable=32):\n",
        "        threshold = 128\n",
        "        data = np.array(img.convert(\"LA\"))\n",
        "        if data[..., -1].var() == 0:\n",
        "            data = (data[..., 0]).astype(np.uint8)\n",
        "        else:\n",
        "            data = (255 - data[..., -1]).astype(np.uint8)\n",
        "        data = (data - data.min()) / (data.max() - data.min()) * 255\n",
        "        if data.mean() > threshold:\n",
        "            gray = 255 * (data < threshold).astype(np.uint8)\n",
        "        else:\n",
        "            gray = 255 * (data > threshold).astype(np.uint8)\n",
        "            data = 255 - data\n",
        "        coords = cv2.findNonZero(gray)\n",
        "        a, b, w, h = cv2.boundingRect(coords)\n",
        "        rect = data[b : b + h, a : a + w]\n",
        "        im = Image.fromarray(rect).convert(\"L\")\n",
        "        dims = []\n",
        "        for x in [w, h]:\n",
        "            div, mod = divmod(x, divable)\n",
        "            dims.append(divable * (div + (1 if mod > 0 else 0)))\n",
        "        padded = Image.new(\"L\", dims, 255)\n",
        "        padded.paste(im, (0, 0, im.size[0], im.size[1]))\n",
        "        return padded\n",
        "\n",
        "    def minmax_size_(self, img, max_dimensions, min_dimensions):\n",
        "        if max_dimensions is not None:\n",
        "            ratios = [a / b for a, b in zip(img.size, max_dimensions)]\n",
        "            if any([r > 1 for r in ratios]):\n",
        "                size = np.array(img.size) // max(ratios)\n",
        "                img = img.resize(tuple(size.astype(int)), Image.BILINEAR)\n",
        "        if min_dimensions is not None:\n",
        "            padded_size = [max(img_dim, min_dim) for img_dim, min_dim in zip(img.size, min_dimensions)]\n",
        "            if padded_size != list(img.size):\n",
        "                padded_im = Image.new(\"L\", padded_size, 255)\n",
        "                padded_im.paste(img, img.getbbox())\n",
        "                img = padded_im\n",
        "        return img\n",
        "\n",
        "    def norm_img_latexocr(self, img):\n",
        "        shape = (1, 1, 3)\n",
        "        mean = [0.7931, 0.7931, 0.7931]\n",
        "        std = [0.1738, 0.1738, 0.1738]\n",
        "        scale = np.float32(1.0 / 255.0)\n",
        "        min_dimensions = [32, 32]\n",
        "        max_dimensions = [672, 192]\n",
        "        mean = np.array(mean).reshape(shape).astype(\"float32\")\n",
        "        std = np.array(std).reshape(shape).astype(\"float32\")\n",
        "        im_h, im_w = img.shape[:2]\n",
        "        if not (min_dimensions[0] <= im_w <= max_dimensions[0] and min_dimensions[1] <= im_h <= max_dimensions[1]):\n",
        "            img = Image.fromarray(np.uint8(img))\n",
        "            img = self.minmax_size_(self.pad_(img), max_dimensions, min_dimensions)\n",
        "            img = np.array(img)\n",
        "            im_h, im_w = img.shape[:2]\n",
        "            img = np.dstack([img, img, img])\n",
        "        img = (img.astype(\"float32\") * scale - mean) / std\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        divide_h = math.ceil(im_h / 16) * 16\n",
        "        divide_w = math.ceil(im_w / 16) * 16\n",
        "        img = np.pad(img, ((0, divide_h - im_h), (0, divide_w - im_w)), constant_values=(1, 1))\n",
        "        img = img[:, :, np.newaxis].transpose(2, 0, 1)\n",
        "        img = img.astype(\"float32\")\n",
        "        return img\n",
        "\n",
        "    def __call__(self, img_list):\n",
        "        img_num = len(img_list)\n",
        "        width_list = [img.shape[1] / float(img.shape[0]) for img in img_list]\n",
        "        indices = np.argsort(np.array(width_list))\n",
        "        rec_res = [[\"\", 0.0]] * img_num\n",
        "        batch_num = self.rec_batch_num\n",
        "        st = time.time()\n",
        "        if self.benchmark:\n",
        "            self.autolog.times.start()\n",
        "\n",
        "        for beg_img_no in range(0, img_num, batch_num):\n",
        "            end_img_no = min(img_num, beg_img_no + batch_num)\n",
        "            norm_img_batch = []\n",
        "            if self.rec_algorithm == \"SRN\":\n",
        "                encoder_word_pos_list = []\n",
        "                gsrm_word_pos_list = []\n",
        "                gsrm_slf_attn_bias1_list = []\n",
        "                gsrm_slf_attn_bias2_list = []\n",
        "            if self.rec_algorithm == \"SAR\":\n",
        "                valid_ratios = []\n",
        "            imgC, imgH, imgW = self.rec_image_shape[:3]\n",
        "            max_wh_ratio = imgW / imgH\n",
        "            wh_ratio_list = []\n",
        "            for ino in range(beg_img_no, end_img_no):\n",
        "                h, w = img_list[indices[ino]].shape[0:2]\n",
        "                wh_ratio = w * 1.0 / h\n",
        "                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
        "                wh_ratio_list.append(wh_ratio)\n",
        "\n",
        "            for ino in range(beg_img_no, end_img_no):\n",
        "                if self.rec_algorithm == \"SAR\":\n",
        "                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n",
        "                    valid_ratios.append(valid_ratio)\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"SRN\":\n",
        "                    norm_img = self.process_image_srn(img_list[indices[ino]], self.rec_image_shape, 8, 25)\n",
        "                    encoder_word_pos_list.append(norm_img[1])\n",
        "                    gsrm_word_pos_list.append(norm_img[2])\n",
        "                    gsrm_slf_attn_bias1_list.append(norm_img[3])\n",
        "                    gsrm_slf_attn_bias2_list.append(norm_img[4])\n",
        "                    norm_img_batch.append(norm_img[0])\n",
        "                elif self.rec_algorithm in [\"SVTR\", \"SATRN\", \"ParseQ\", \"CPPD\"]:\n",
        "                    norm_img = self.resize_norm_img_svtr(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm in [\"CPPDPadding\"]:\n",
        "                    norm_img = self.resize_norm_img_cppd_padding(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm in [\"VisionLAN\", \"PREN\"]:\n",
        "                    norm_img = self.resize_norm_img_vl(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"SPIN\":\n",
        "                    norm_img = self.resize_norm_img_spin(img_list[indices[ino]])\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"ABINet\":\n",
        "                    norm_img = self.resize_norm_img_abinet(img_list[indices[ino]], self.rec_image_shape)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                elif self.rec_algorithm == \"RobustScanner\":\n",
        "                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(\n",
        "                        img_list[indices[ino]],\n",
        "                        self.rec_image_shape,\n",
        "                        width_downsample_ratio=0.25,\n",
        "                    )\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n",
        "                    valid_ratios = [] if 'valid_ratios' not in locals() else valid_ratios\n",
        "                    valid_ratios.append(valid_ratio)\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                    word_positions_list = [] if 'word_positions_list' not in locals() else word_positions_list\n",
        "                    word_positions = np.array(range(0, 40)).astype(\"int64\")\n",
        "                    word_positions = np.expand_dims(word_positions, axis=0)\n",
        "                    word_positions_list.append(word_positions)\n",
        "                elif self.rec_algorithm == \"CAN\":\n",
        "                    norm_img = self.norm_img_can(img_list[indices[ino]], max_wh_ratio)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                    norm_image_mask = np.ones(norm_img.shape, dtype=\"float32\")\n",
        "                    word_label = np.ones([1, 36], dtype=\"int64\")\n",
        "                    norm_img_mask_batch = [] if 'norm_img_mask_batch' not in locals() else norm_img_mask_batch\n",
        "                    word_label_list = [] if 'word_label_list' not in locals() else word_label_list\n",
        "                    norm_img_mask_batch.append(norm_image_mask)\n",
        "                    word_label_list.append(word_label)\n",
        "                elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "                    norm_img = self.norm_img_latexocr(img_list[indices[ino]])\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "                else:\n",
        "                    norm_img = self.resize_norm_img(img_list[indices[ino]], max_wh_ratio)\n",
        "                    norm_img = norm_img[np.newaxis, :]\n",
        "                    norm_img_batch.append(norm_img)\n",
        "\n",
        "            norm_img_batch = np.concatenate(norm_img_batch)\n",
        "            norm_img_batch = norm_img_batch.copy()\n",
        "            norm_img_batch = norm_img_batch.astype(np.float32)  # FORCE float32\n",
        "\n",
        "            if self.benchmark:\n",
        "                self.autolog.times.stamp()\n",
        "\n",
        "            if self.rec_algorithm == \"SRN\":\n",
        "                encoder_word_pos_list = np.concatenate(encoder_word_pos_list)\n",
        "                gsrm_word_pos_list = np.concatenate(gsrm_word_pos_list)\n",
        "                gsrm_slf_attn_bias1_list = np.concatenate(gsrm_slf_attn_bias1_list)\n",
        "                gsrm_slf_attn_bias2_list = np.concatenate(gsrm_slf_attn_bias2_list)\n",
        "                inputs = [norm_img_batch, encoder_word_pos_list, gsrm_word_pos_list, gsrm_slf_attn_bias1_list, gsrm_slf_attn_bias2_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = {\"predict\": outputs[2]}\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = {\"predict\": outputs[2]}\n",
        "            elif self.rec_algorithm == \"SAR\":\n",
        "                valid_ratios = np.concatenate(valid_ratios).astype(np.float32)\n",
        "                inputs = [norm_img_batch, np.array([valid_ratios], dtype=np.float32).T]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0]\n",
        "            elif self.rec_algorithm == \"RobustScanner\":\n",
        "                valid_ratios = np.concatenate(valid_ratios).astype(np.float32)\n",
        "                word_positions_list = np.concatenate(word_positions_list).astype(np.int64)\n",
        "                inputs = [norm_img_batch, valid_ratios, word_positions_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0]\n",
        "            elif self.rec_algorithm == \"CAN\":\n",
        "                norm_img_mask_batch = np.concatenate(norm_img_mask_batch)\n",
        "                word_label_list = np.concatenate(word_label_list)\n",
        "                inputs = [norm_img_batch, norm_img_mask_batch, word_label_list]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs\n",
        "            elif self.rec_algorithm == \"LaTeXOCR\":\n",
        "                inputs = [norm_img_batch]\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs\n",
        "                else:\n",
        "                    input_names = self.predictor.get_input_names()\n",
        "                    for i in range(len(input_names)):\n",
        "                        self.predictor.get_input_handle(input_names[i]).copy_from_cpu(inputs[i])\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs\n",
        "            else:\n",
        "                if self.use_onnx:\n",
        "                    input_dict = {self.input_tensor.name: norm_img_batch}\n",
        "                    outputs = self.predictor.run(self.output_tensors, input_dict)\n",
        "                    preds = outputs[0]\n",
        "                else:\n",
        "                    self.input_tensor.copy_from_cpu(norm_img_batch)\n",
        "                    self.predictor.run()\n",
        "                    outputs = [ot.copy_to_cpu() for ot in self.output_tensors]\n",
        "                    preds = outputs[0] if len(outputs) == 1 else outputs\n",
        "\n",
        "            if self.postprocess_params[\"name\"] == \"CTCLabelDecode\":\n",
        "                rec_result = self.postprocess_op(preds, return_word_box=self.return_word_box, wh_ratio_list=wh_ratio_list, max_wh_ratio=max_wh_ratio)\n",
        "            elif self.postprocess_params[\"name\"] == \"LaTeXOCRDecode\":\n",
        "                preds = [p.reshape([-1]) for p in preds]\n",
        "                rec_result = self.postprocess_op(preds)\n",
        "            else:\n",
        "                rec_result = self.postprocess_op(preds)\n",
        "\n",
        "            for rno in range(len(rec_result)):\n",
        "                rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n",
        "            if self.benchmark:\n",
        "                self.autolog.times.end(stamp=True)\n",
        "\n",
        "        return rec_res, time.time() - st\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    image_file_list = get_image_file_list(args.image_dir)\n",
        "    valid_image_file_list = []\n",
        "    img_list = []\n",
        "    log_file = args.save_log_path\n",
        "    if os.path.isdir(args.save_log_path) or (not os.path.exists(args.save_log_path) and args.save_log_path.endswith(\"/\")):\n",
        "        log_file = os.path.join(log_file, \"benchmark_recognition.log\")\n",
        "    logger = get_logger(log_file=log_file)\n",
        "    logger.info(\"In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\")\n",
        "    text_recognizer = TextRecognizer(args)\n",
        "    if args.warmup:\n",
        "        img = np.random.uniform(0, 255, [48, 320, 3]).astype(np.uint8)\n",
        "        for i in range(2):\n",
        "            res = text_recognizer([img] * int(args.rec_batch_num))\n",
        "    for image_file in image_file_list:\n",
        "        img, flag, _ = check_and_read(image_file)\n",
        "        if not flag:\n",
        "            img = cv2.imread(image_file)\n",
        "        if img is None:\n",
        "            logger.info(\"error in loading image:{}\".format(image_file))\n",
        "            continue\n",
        "        valid_image_file_list.append(image_file)\n",
        "        img_list.append(img)\n",
        "    try:\n",
        "        rec_res, _ = text_recognizer(img_list)\n",
        "    except Exception as E:\n",
        "        logger.info(traceback.format_exc())\n",
        "        logger.info(E)\n",
        "        exit()\n",
        "    for ino in range(len(img_list)):\n",
        "        logger.info(\"Predicts of {}:{}\".format(valid_image_file_list[ino], rec_res[ino]))\n",
        "    if args.benchmark:\n",
        "        text_recognizer.autolog.report()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(utility.parse_args())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Dx-mTeQf9MUF",
        "outputId": "7b654754-77d6-46ef-b648-c2bb9dec8460"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-328432007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0m__dir__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "This code is refer from:\n",
        "https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/encoders/channel_reduction_encoder.py\n",
        "https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/robust_scanner_decoder.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import paddle\n",
        "from paddle import ParamAttr\n",
        "import paddle.nn as nn\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "\n",
        "class BaseDecoder(nn.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, img_metas):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward_test(self, feat, out_enc, img_metas):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        feat,\n",
        "        out_enc,\n",
        "        label=None,\n",
        "        valid_ratios=None,\n",
        "        word_positions=None,\n",
        "        train_mode=True,\n",
        "    ):\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        if train_mode:\n",
        "            return self.forward_train(\n",
        "                feat, out_enc, label, valid_ratios, word_positions\n",
        "            )\n",
        "        return self.forward_test(feat, out_enc, valid_ratios, word_positions)\n",
        "\n",
        "\n",
        "class ChannelReductionEncoder(nn.Layer):\n",
        "    \"\"\"Change the channel number with a one by one convoluational layer.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(ChannelReductionEncoder, self).__init__()\n",
        "\n",
        "        self.layer = nn.Conv2D(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            weight_attr=nn.initializer.XavierNormal(),\n",
        "        )\n",
        "\n",
        "    def forward(self, feat):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Image features with the shape of\n",
        "                :math:`(N, C_{in}, H, W)`.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A tensor of shape :math:`(N, C_{out}, H, W)`.\n",
        "        \"\"\"\n",
        "        return self.layer(feat)\n",
        "\n",
        "\n",
        "def masked_fill(x, mask, value):\n",
        "    y = paddle.full(x.shape, value, x.dtype)\n",
        "    return paddle.where(mask, y, x)\n",
        "\n",
        "\n",
        "class DotProductAttentionLayer(nn.Layer):\n",
        "    def __init__(self, dim_model=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.scale = dim_model**-0.5 if dim_model is not None else 1.0\n",
        "\n",
        "    def forward(self, query, key, value, h, w, valid_ratios=None):\n",
        "        # query: [N, D_m, Lq]\n",
        "        # key: [N, C_enc, Lk]  where Lk == h*w\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        logits = paddle.matmul(query, key) * self.scale\n",
        "        n, c, t = logits.shape\n",
        "        # reshape to (n, c, h, w)\n",
        "        logits = paddle.reshape(logits, [n, c, h, w])\n",
        "\n",
        "        if valid_ratios is not None:\n",
        "            # vectorized mask: set logits[..., valid_width:] = -inf per sample\n",
        "            # valid_ratios: Tensor of shape [N] or [N,1]\n",
        "            valid_ratios = paddle.reshape(valid_ratios, [-1])\n",
        "            widths = paddle.cast(paddle.round(valid_ratios * w), dtype='int64')\n",
        "            widths = paddle.clip(widths, min=0, max=w)\n",
        "\n",
        "            N = widths.shape[0]\n",
        "            W = logits.shape[-1]\n",
        "\n",
        "            arange = paddle.arange(W, dtype='int64')\n",
        "            arange = paddle.reshape(arange, [1, W])\n",
        "            arange = arange.expand([N, W])\n",
        "            widths_expand = paddle.reshape(widths, [N, 1]).expand([N, W])\n",
        "\n",
        "            mask = arange >= widths_expand  # [N, W]\n",
        "            mask = paddle.reshape(mask, [N, 1, 1, W])  # [N,1,1,W]\n",
        "\n",
        "            neg_inf = paddle.full([1], float('-inf'), dtype=logits.dtype)\n",
        "            # apply mask: positions >= valid_width -> -inf\n",
        "            logits = paddle.where(mask, neg_inf, logits)\n",
        "\n",
        "        # reshape to (n, c, t)\n",
        "        logits = paddle.reshape(logits, [n, c, t])\n",
        "        weights = F.softmax(logits, axis=2)\n",
        "        value = paddle.transpose(value, (0, 2, 1))\n",
        "        glimpse = paddle.matmul(weights, value)\n",
        "        glimpse = paddle.transpose(glimpse, (0, 2, 1))\n",
        "        return glimpse\n",
        "\n",
        "\n",
        "class SequenceAttentionDecoder(BaseDecoder):\n",
        "    \"\"\"Sequence attention decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        rnn_layers (int): Number of RNN layers.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        start_idx (int): The index of `<SOS>`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        padding_idx (int): The index of `<PAD>`.\n",
        "        dropout (float): Dropout rate.\n",
        "        return_feature (bool): Return feature or logits as the result.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss as specified in\n",
        "        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        rnn_layers=2,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        max_seq_len=40,\n",
        "        start_idx=0,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        dropout=0,\n",
        "        return_feature=False,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.return_feature = return_feature\n",
        "        self.encode_value = encode_value\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.start_idx = start_idx\n",
        "        self.mask = mask\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            self.num_classes, self.dim_model, padding_idx=padding_idx\n",
        "        )\n",
        "\n",
        "        self.sequence_layer = nn.LSTM(\n",
        "            input_size=dim_model,\n",
        "            hidden_size=dim_model,\n",
        "            num_layers=rnn_layers,\n",
        "            time_major=False,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.attention_layer = DotProductAttentionLayer()\n",
        "\n",
        "        self.prediction = None\n",
        "        if not self.return_feature:\n",
        "            pred_num_classes = num_classes - 1\n",
        "            self.prediction = nn.Linear(\n",
        "                dim_model if encode_value else dim_input, pred_num_classes\n",
        "            )\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, valid_ratios):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            targets (Tensor): a tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it would be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "\n",
        "        tgt_embedding = self.embedding(targets)\n",
        "\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, len_q, c_q = tgt_embedding.shape\n",
        "        assert c_q == self.dim_model\n",
        "        assert len_q <= self.max_seq_len\n",
        "\n",
        "        query, _ = self.sequence_layer(tgt_embedding)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n",
        "        if self.encode_value:\n",
        "            value = key\n",
        "        else:\n",
        "            value = paddle.reshape(feat, [n, c_feat, h * w])\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        out = self.prediction(attn_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output logit sequence tensor of shape\n",
        "            :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        seq_len = self.max_seq_len\n",
        "        batch_size = feat.shape[0]\n",
        "\n",
        "        decode_sequence = (\n",
        "            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n",
        "        )\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            step_out = self.forward_test_step(\n",
        "                feat, out_enc, decode_sequence, i, valid_ratios\n",
        "            )\n",
        "            outputs.append(step_out)\n",
        "            max_idx = paddle.argmax(step_out, axis=1, keepdim=False)\n",
        "            if i < seq_len - 1:\n",
        "                decode_sequence[:, i + 1] = max_idx\n",
        "\n",
        "        outputs = paddle.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward_test_step(\n",
        "        self, feat, out_enc, decode_sequence, current_step, valid_ratios\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            decode_sequence (Tensor): Shape :math:`(N, T)`. The tensor that\n",
        "                stores history decoding result.\n",
        "            current_step (int): Current decoding step.\n",
        "            valid_ratios (Tensor): valid length ratio of img\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Shape :math:`(N, C-1)`. The logit tensor of predicted\n",
        "            tokens at current time step.\n",
        "        \"\"\"\n",
        "\n",
        "        embed = self.embedding(decode_sequence)\n",
        "\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, _, c_q = embed.shape\n",
        "        assert c_q == self.dim_model\n",
        "\n",
        "        query, _ = self.sequence_layer(embed)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n",
        "        if self.encode_value:\n",
        "            value = key\n",
        "        else:\n",
        "            value = paddle.reshape(feat, [n, c_feat, h * w])\n",
        "\n",
        "        # [n, c, l]\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        out = attn_out[:, :, current_step]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return out\n",
        "\n",
        "        out = self.prediction(out)\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionAwareLayer(nn.Layer):\n",
        "    def __init__(self, dim_model, rnn_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=dim_model,\n",
        "            hidden_size=dim_model,\n",
        "            num_layers=rnn_layers,\n",
        "            time_major=False,\n",
        "        )\n",
        "\n",
        "        self.mixer = nn.Sequential(\n",
        "            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img_feature):\n",
        "        n, c, h, w = img_feature.shape\n",
        "        rnn_input = paddle.transpose(img_feature, (0, 2, 3, 1))\n",
        "        rnn_input = paddle.reshape(rnn_input, (n * h, w, c))\n",
        "        rnn_output, _ = self.rnn(rnn_input)\n",
        "        rnn_output = paddle.reshape(rnn_output, (n, h, w, c))\n",
        "        rnn_output = paddle.transpose(rnn_output, (0, 3, 1, 2))\n",
        "        out = self.mixer(rnn_output)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionAttentionDecoder(BaseDecoder):\n",
        "    \"\"\"Position attention decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        rnn_layers (int): Number of RNN layers.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        return_feature (bool): Return feature or logits as the result.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        rnn_layers=2,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        max_seq_len=40,\n",
        "        mask=True,\n",
        "        return_feature=False,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.return_feature = return_feature\n",
        "        self.encode_value = encode_value\n",
        "        self.mask = mask\n",
        "\n",
        "        self.embedding = nn.Embedding(self.max_seq_len + 1, self.dim_model)\n",
        "\n",
        "        self.position_aware_module = PositionAwareLayer(self.dim_model, rnn_layers)\n",
        "\n",
        "        self.attention_layer = DotProductAttentionLayer()\n",
        "\n",
        "        self.prediction = None\n",
        "        if not self.return_feature:\n",
        "            pred_num_classes = num_classes - 1\n",
        "            self.prediction = nn.Linear(\n",
        "                dim_model if encode_value else dim_input, pred_num_classes\n",
        "            )\n",
        "\n",
        "    def _get_position_index(self, length, batch_size):\n",
        "        position_index_list = []\n",
        "        for i in range(batch_size):\n",
        "            position_index = paddle.arange(0, end=length, step=1, dtype=\"int64\")\n",
        "            position_index_list.append(position_index)\n",
        "        batch_position_index = paddle.stack(position_index_list, axis=0)\n",
        "        return batch_position_index\n",
        "\n",
        "    def forward_train(self, feat, out_enc, targets, valid_ratios, position_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            targets (dict): A dict with the key ``padded_targets``, a\n",
        "                tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor): valid length ratio of img.\n",
        "            position_index (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it will be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "        _, len_q = targets.shape\n",
        "        assert len_q <= self.max_seq_len\n",
        "\n",
        "        position_out_enc = self.position_aware_module(out_enc)\n",
        "\n",
        "        query = self.embedding(position_index)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n",
        "        if self.encode_value:\n",
        "            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n",
        "        else:\n",
        "            value = paddle.reshape(feat, (n, c_feat, h * w))\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        return self.prediction(attn_out)\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios, position_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor): valid length ratio of img\n",
        "            position_index (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n",
        "            ``return_feature=False``. Otherwise it would be the hidden feature\n",
        "            before the prediction projection layer, whose shape is\n",
        "            :math:`(N, T, D_m)`.\n",
        "        \"\"\"\n",
        "        n, c_enc, h, w = out_enc.shape\n",
        "        assert c_enc == self.dim_model\n",
        "        _, c_feat, _, _ = feat.shape\n",
        "        assert c_feat == self.dim_input\n",
        "\n",
        "        position_out_enc = self.position_aware_module(out_enc)\n",
        "\n",
        "        query = self.embedding(position_index)\n",
        "        query = paddle.transpose(query, (0, 2, 1))\n",
        "        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n",
        "        if self.encode_value:\n",
        "            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n",
        "        else:\n",
        "            value = paddle.reshape(feat, (n, c_feat, h * w))\n",
        "\n",
        "        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n",
        "        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n",
        "\n",
        "        if self.return_feature:\n",
        "            return attn_out\n",
        "\n",
        "        return self.prediction(attn_out)\n",
        "\n",
        "\n",
        "class RobustScannerFusionLayer(nn.Layer):\n",
        "    def __init__(self, dim_model, dim=-1):\n",
        "        super(RobustScannerFusionLayer, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim = dim\n",
        "        self.linear_layer = nn.Linear(dim_model * 2, dim_model * 2)\n",
        "\n",
        "    def forward(self, x0, x1):\n",
        "        # Only run Python-level shape assertion in dynamic mode (training/debug).\n",
        "        # During static export, evaluating tensor-based booleans causes dy2static\n",
        "        # to fail. So skip the assert in static mode.\n",
        "        if paddle.in_dynamic_mode():\n",
        "            assert x0.shape == x1.shape, f\"x0.shape ({x0.shape}) != x1.shape ({x1.shape})\"\n",
        "\n",
        "        fusion_input = paddle.concat([x0, x1], self.dim)\n",
        "        output = self.linear_layer(fusion_input)\n",
        "        output = F.glu(output, self.dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "class RobustScannerDecoder(BaseDecoder):\n",
        "    \"\"\"Decoder for RobustScanner.\n",
        "\n",
        "    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n",
        "    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes :math:`C`.\n",
        "        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n",
        "        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n",
        "            same as encoder output vector ``out_enc``.\n",
        "        max_seq_len (int): Maximum output sequence length :math:`T`.\n",
        "        start_idx (int): The index of `<SOS>`.\n",
        "        mask (bool): Whether to mask input features according to\n",
        "            ``img_meta['valid_ratio']``.\n",
        "        padding_idx (int): The index of `<PAD>`.\n",
        "        encode_value (bool): Whether to use the output of encoder ``out_enc``\n",
        "            as `value` of attention layer. If False, the original feature\n",
        "            ``feat`` will be used.\n",
        "\n",
        "    Warning:\n",
        "        This decoder will not predict the final class which is assumed to be\n",
        "        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n",
        "        is also ignored by loss as specified in\n",
        "        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=None,\n",
        "        dim_input=512,\n",
        "        dim_model=128,\n",
        "        hybrid_decoder_rnn_layers=2,\n",
        "        hybrid_decoder_dropout=0,\n",
        "        position_decoder_rnn_layers=2,\n",
        "        max_seq_len=40,\n",
        "        start_idx=0,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        encode_value=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_model = dim_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.encode_value = encode_value\n",
        "        self.start_idx = start_idx\n",
        "        self.padding_idx = padding_idx\n",
        "        self.mask = mask\n",
        "\n",
        "        # init hybrid decoder\n",
        "        self.hybrid_decoder = SequenceAttentionDecoder(\n",
        "            num_classes=num_classes,\n",
        "            rnn_layers=hybrid_decoder_rnn_layers,\n",
        "            dim_input=dim_input,\n",
        "            dim_model=dim_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            start_idx=start_idx,\n",
        "            mask=mask,\n",
        "            padding_idx=padding_idx,\n",
        "            dropout=hybrid_decoder_dropout,\n",
        "            encode_value=encode_value,\n",
        "            return_feature=True,\n",
        "        )\n",
        "\n",
        "        # init position decoder\n",
        "        self.position_decoder = PositionAttentionDecoder(\n",
        "            num_classes=num_classes,\n",
        "            rnn_layers=position_decoder_rnn_layers,\n",
        "            dim_input=dim_input,\n",
        "            dim_model=dim_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            mask=mask,\n",
        "            encode_value=encode_value,\n",
        "            return_feature=True,\n",
        "        )\n",
        "\n",
        "        self.fusion_module = RobustScannerFusionLayer(\n",
        "            self.dim_model if encode_value else dim_input\n",
        "        )\n",
        "\n",
        "        pred_num_classes = num_classes - 1\n",
        "        self.prediction = nn.Linear(\n",
        "            dim_model if encode_value else dim_input, pred_num_classes\n",
        "        )\n",
        "\n",
        "    def forward_train(self, feat, out_enc, target, valid_ratios, word_positions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            target (dict): A dict with the key ``padded_targets``, a\n",
        "                tensor of shape :math:`(N, T)`. Each element is the index of a\n",
        "                character.\n",
        "            valid_ratios (Tensor):\n",
        "            word_positions (Tensor): The position of each word.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        hybrid_glimpse = self.hybrid_decoder.forward_train(\n",
        "            feat, out_enc, target, valid_ratios\n",
        "        )\n",
        "        position_glimpse = self.position_decoder.forward_train(\n",
        "            feat, out_enc, target, valid_ratios, word_positions\n",
        "        )\n",
        "\n",
        "        fusion_out = self.fusion_module(hybrid_glimpse, position_glimpse)\n",
        "\n",
        "        out = self.prediction(fusion_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward_test(self, feat, out_enc, valid_ratios, word_positions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n",
        "            out_enc (Tensor): Encoder output of shape\n",
        "                :math:`(N, D_m, H, W)`.\n",
        "            valid_ratios (Tensor):\n",
        "            word_positions (Tensor): The position of each word.\n",
        "        Returns:\n",
        "            Tensor: The output logit sequence tensor of shape\n",
        "            :math:`(N, T, C-1)`.\n",
        "        \"\"\"\n",
        "        seq_len = self.max_seq_len\n",
        "        batch_size = feat.shape[0]\n",
        "\n",
        "        decode_sequence = (\n",
        "            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n",
        "        )\n",
        "\n",
        "        position_glimpse = self.position_decoder.forward_test(\n",
        "            feat, out_enc, valid_ratios, word_positions\n",
        "        )\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(seq_len):\n",
        "            hybrid_glimpse_step = self.hybrid_decoder.forward_test_step(\n",
        "                feat, out_enc, decode_sequence, i, valid_ratios\n",
        "            )\n",
        "\n",
        "            fusion_out = self.fusion_module(\n",
        "                hybrid_glimpse_step, position_glimpse[:, i, :]\n",
        "            )\n",
        "\n",
        "            char_out = self.prediction(fusion_out)\n",
        "            char_out = F.softmax(char_out, -1)\n",
        "            outputs.append(char_out)\n",
        "            max_idx = paddle.argmax(char_out, axis=1, keepdim=False)\n",
        "            if i < seq_len - 1:\n",
        "                decode_sequence[:, i + 1] = max_idx\n",
        "\n",
        "        outputs = paddle.stack(outputs, 1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class RobustScannerHead(nn.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_channels,  # 90 + unknown + start + padding\n",
        "        in_channels,\n",
        "        enc_outchannles=128,\n",
        "        hybrid_dec_rnn_layers=2,\n",
        "        hybrid_dec_dropout=0,\n",
        "        position_dec_rnn_layers=2,\n",
        "        start_idx=0,\n",
        "        max_text_length=40,\n",
        "        mask=True,\n",
        "        padding_idx=None,\n",
        "        encode_value=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(RobustScannerHead, self).__init__()\n",
        "\n",
        "        # encoder module\n",
        "        self.encoder = ChannelReductionEncoder(\n",
        "            in_channels=in_channels, out_channels=enc_outchannles\n",
        "        )\n",
        "\n",
        "        # decoder module\n",
        "        self.decoder = RobustScannerDecoder(\n",
        "            num_classes=out_channels,\n",
        "            dim_input=in_channels,\n",
        "            dim_model=enc_outchannles,\n",
        "            hybrid_decoder_rnn_layers=hybrid_dec_rnn_layers,\n",
        "            hybrid_decoder_dropout=hybrid_dec_dropout,\n",
        "            position_decoder_rnn_layers=position_dec_rnn_layers,\n",
        "            max_seq_len=max_text_length,\n",
        "            start_idx=start_idx,\n",
        "            mask=mask,\n",
        "            padding_idx=padding_idx,\n",
        "            encode_value=encode_value,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        \"\"\"\n",
        "        targets: [label, valid_ratio, word_positions]\n",
        "        \"\"\"\n",
        "        out_enc = self.encoder(inputs)\n",
        "        valid_ratios = None\n",
        "        word_positions = targets[-1]\n",
        "\n",
        "        if len(targets) > 1:\n",
        "            valid_ratios = targets[-2]\n",
        "\n",
        "        if self.training:\n",
        "            label = targets[0]  # label\n",
        "            label = paddle.to_tensor(label, dtype=\"int64\")\n",
        "            final_out = self.decoder(\n",
        "                inputs, out_enc, label, valid_ratios, word_positions\n",
        "            )\n",
        "        else:\n",
        "            # For static export, if some tensor-based control flow still exists\n",
        "            # in downstream modules, fallback to disabling valid_ratios.\n",
        "            # But since attention is vectorized above, we pass valid_ratios as-is.\n",
        "            final_out = self.decoder(\n",
        "                inputs,\n",
        "                out_enc,\n",
        "                label=None,\n",
        "                valid_ratios=valid_ratios,\n",
        "                word_positions=word_positions,\n",
        "                train_mode=False,\n",
        "            )\n",
        "        return final_out\n"
      ],
      "metadata": {
        "id": "Q8fXMOTo-R7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}